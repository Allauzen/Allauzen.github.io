<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-06-24 lun. 17:13 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Softmax Gradient</title>
<meta name="author" content="A. Allauzen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'left',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'left',
      displayIndent: '5em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'left',
      displayIndent: '5em'
    },
    output: {
      font: 'mathjax-euler',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Softmax Gradient</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgd76444a">1. Conditional maximum likelihood</a></li>
<li><a href="#org61cadd1">2. Gradient of the softmax function w.r.t to parameters</a></li>
</ul>
</div>
</div>
<hr>


<p>
<div style="display: none">
\(
\global\def\a{a}
\global\def\nclasses{N}
\global\def\nfeats{D}
\global\def\gold{\tilde{k}}
\global\def\W{\mathbf{W}}
\global\def\w{\mathbf{w}}
\global\def\x{\mathbf{x}}
\global\def\pa{\boldsymbol{\theta}}
\global\newcommand\loss{\mathcal{L}(\pa)}
\global\newcommand\Z{Z(\pa)}
\)
</div>
</p>

<p>
The Softmax function is widely used in many machine learning models: maximum-entropy, as an activation function for the last layer of neural networks, &#x2026;  Assume, for instance, a classification task with \(\nclasses\) classes on output. To infer a probability distribution over these classes given an input vector \(\x\) of dimension \(\nfeats\), the softmax function is a common choice. The random variable \(C\) denotes the class to assign and its outcome set is a range of integers : \(c \in [1:\nclasses]\). For simplicity, the mention to this random variable is dropped and \(P(k|\x)\) is the probability of \(C=k\) given \(\x\). This distribution is parametrized by a set of parameters \(\pa\). We also omit the bias (or intercept) terms for the sake of clarity. By the way, you can assume an additional input set to 1. 
</p>

<div class="org-center">
\begin{align}
\x &\in \mathbb{R}^{\nfeats}\\
\w_k &\in \mathbb{R}^{\nfeats},\textrm{parameter vector for class }k,\  \forall k \in [ 1:\nclasses] \\
\a_k &= \w_k^t \x,\textrm{ the dot product between } \w_k \textrm{ and } \x \textrm{ or the pre-activation function for NNet.}\\
P(c=k|\x) &= \frac{exp(\a_k)}{\sum_{k'} exp(\a_{k'})}, \textrm{ the probability of the class $k$ given the input }\x.\\
&= \frac{\exp(\a_k)}{\Z}, \textrm{ where $\Z$ is the normalization (or partition function)}. \\
log P(C=k|\x) &= \a_k - log(\Z), \textrm{ in its log form}.
\end{align}
</div>

<div id="outline-container-orgd76444a" class="outline-2">
<h2 id="orgd76444a"><span class="section-number-2">1.</span> Conditional maximum likelihood</h2>
<div class="outline-text-2" id="text-1">
<p>
First, let us define the loss function for one training example:
</p>
<div class="org-center">
<p>
\[
\loss = -log( P(k=\gold|\x)),
\]
</p>
</div>
<p>
where \(\gold\) is the supervision information, the right class to predict, the gold-standard.  
With several training examples, just take the sum of their log probabilities.
</p>

<p>
This loss function that we want to minimize has been called the log-loss or the cross-entropy loss. This loss function is simply the negative log-likelihood. Its minimization is equivalent to maximize the likelihood of the paramaters on a given training set.  When considering the whole training set, there is no closed solution for this optimization problem and therefore the gradient descent is an efficient solution.  In an online training scenario, the parameters are therefore updated to increase the log-likelihood for a given training example.
</p>
</div>
</div>

<div id="outline-container-org61cadd1" class="outline-2">
<h2 id="org61cadd1"><span class="section-number-2">2.</span> Gradient of the softmax function w.r.t to parameters</h2>
<div class="outline-text-2" id="text-2">
<p>
The parameter set \(\pa\) gathers all the vectors \(\w_k\). This is a matrix \(\W\) in which \(w_{kj}\) is the cell of row \(k\) and column \(j\), a.k.a the component \(j\) of the vector associated to the class \(k\). Let consider the gradient of the loss function w.r.t to one parameter value \(w_{kj}\):
</p>
<div class="org-center">
\begin{align}
\frac{\partial \loss}{\partial w_{kj}} &= \nabla_{kj} \\
&= \frac{\partial a_{\gold}}{\partial w_{kj}} - \frac{\partial log(\Z)}{\partial w_{kj}}\\
&= \frac{\partial a_{\gold}}{\partial w_{kj}} - \frac{\partial log(\sum_{k'} exp(\a_{k'}))}{\partial w_{kj}}
\end{align}
</div>
<p>
Note that for a given \(k\) and \(\forall j\), only \(a_k\) depends on \(k\), so for the first term there are two cases:
</p>
<ul class="org-ul">
<li>\(k=\gold\): \(\frac{\partial a_{\gold}}{\partial w_{kj}} = x_j\) because \(\w_k^t \x  = \sum_{j'} w_{kj} x_{j'}\)</li>
<li>\(k\ne\gold\): \(\frac{\partial a_{\gold}}{\partial w_{kj}} = 0\)</li>
</ul>
<p>
Let us define the Kronecker delta function \(\delta_{k,k'}\) as: 
</p>
<ul class="org-ul">
<li>\(\delta_{k,k'}= 1\) if and only if \(k=k'\),</li>
<li>\(\delta_{k,k'}= 0\) otherwise.</li>
</ul>

<p>
Then we can compactly write: \[ \frac{\partial a_{\gold}}{\partial w_{kj}} = \delta_{k,\gold}x_j.\] 
Now, consider the derivative of the log-partition term.
</p>
\begin{align}
\frac{\partial log(\Z)}{\partial w_{kj}}&= \frac{\partial log(\Z)}{\partial \Z}\frac{\Z}{\partial w_{kj}} = \frac{1}{\Z}  \frac{\partial \Z}{\partial w_{kj}} \\
\frac{\partial \Z}{\partial w_{kj}}  &= \frac{\partial\sum_{k'} exp(\a_{k'})}{\partial w_{kj}} = \frac{\partial exp(\a_{k})}{\partial w_{kj}} =  exp(\a_{k})\frac{\partial \a_{k}}{\partial w_{kj}} \\
&= x_j exp(\a_{k}) \\
\frac{\partial log(\Z)}{\partial w_{kj}}&= \frac{exp(\a_{k})}{\Z}x_j = x_j P(k|\x)
\end{align}

<p>
Now, we merge the two terms, rearrange a bit to get : 
\[\frac{\partial \loss}{\partial w_{kj}} = - (\delta_{k,\gold} - P(k|\x))x_j\]
Note that the term \((\delta_{k,\gold} - P(k|\x))\) can be interpreted as the prediction error made by the model \((P(k|\x)\) knowing the expected answer \((delta_{k,\gold})\). 
The form of the gradient is very similar to the one  of the logistic regression. 
</p>

<p>
The Maximum-Likelihood estimate of the parameters \(\W\) acts as a memorization of the dataset: for a given class \(k\) the weight vector of the class \(\w_k\) is a weighted sum of the training examples, weighted by the error. 
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-07-27 (last update 2022-05-09)</p>
<p class="author">Author: A. Allauzen</p>
<p class="date">Created: 2024-06-24 lun. 17:13</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
