---
title: "Variational Bayes : some basics"
date: 2017-07-21
layout: post
categories: 
tags: 
- article 
- bayes 
- variational 
- EM
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org9560d9a">1. Variational Bayes: approximate inference</a></li>
<li><a href="#orgd9deebe">2. The EM view</a>
<ul>
<li><a href="#orgefdcc8d">2.1. E step</a></li>
<li><a href="#org7d7fbe8">2.2. M step</a></li>
<li><a href="#org163e935">2.3. A summary of EM</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>
<p>
\(\newcommand{\X}{\mathbf{X}}\)
\(\newcommand{\Z}{\mathbf{Z}}\)
\(\newcommand{\pa}{\mathbf{\theta}}\)
\(\newcommand{\paold}{\pa^{old}}\)
\(\newcommand{\panew}{\pa^{new}}\)
</p>

<p>
\(\newcommand{\lb}{\mathcal{L}(q;\pa)}\) 
</p>

<p>
\(\newcommand{\dkl}{D_{kl}}\)
</p>


<p>
Consider a probabilistic model in which we denote all of the observed variables by \(\X\) and all and all of the hidden variables by \(\Z\). In general, the probabilistic model is designed by a generative story which implies a definition of the joint probability \(P(\X, \Z|\pa)\) or \(P(\X, \Z,\pa)\) in the real bayesian view. 
</p>

<p>
If we consider the joint distribution \(P(\X, \Z|\pa)\), it is defined by a set of parameters denoted \(\pa\). The goal is to maximize the likelihood function \(P(\X|\pa)\) to learn the parameters \(\pa\). The problem is that this maximization problem is in most of the case intractable.  
</p>

<p>
If we consider \(P(\X, \Z,\pa)\), there are different level of bayesian inference: 
</p>
<ul class="org-ul">
<li>infer  \(P(\pa|\X)\) which also implies a summation of \(\Z\)</li>
<li>infer \(P(\X)\) for prediction purpose</li>
<li>infer \(P(\Z|\X,\pa)\), the posterior distribution of the latent variable to better analyse new data.</li>
</ul>

<p>
In all cases, the inference is untractable because we need to estimate \(P(\X|\pa)\) or even marginalize parameters. This requires approximate inference approach, such as MCMC or Variational one.
</p>

<div id="outline-container-org9560d9a" class="outline-2">
<h2 id="org9560d9a"><span class="section-number-2">1</span> Variational Bayes: approximate inference</h2>
<div class="outline-text-2" id="text-1">
<p>
This likelihood can be expressed in two ways: \(P(\X|\pa)= \sum_{\Z} P(\X,\Z|\pa)= \frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)}\). Let us consider the second one in its log formulation:
</p>


<div class="org-center">
<p>
\(log(P(\X|\pa)= log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)})\)
</p>
</div>

<p>
Next we introduce a distribution \(q(\Z)\) defined over the latent variables by multliplying both parts, and we can also sum over \(\Z\) 
</p>
<div class="org-center">
<p>
\(\sum_{\Z} q(\Z) log(P(\X|\pa)= \sum_{\Z}log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)})q(\Z)\)
</p>
</div>

<p>
The right hand side is just \(P(\X|\pa)\) and remains unchanged. The second trick is to introduce \(q(\Z)\) in the fraction:
</p>
<div class="org-center">
<p>
\(log(P(\X|\pa))= \sum_{\Z}q(\Z) log(\frac{P(\X,\Z|\pa)q(\Z)}{P(\Z|\X,\pa)q(\Z)})\)
</p>
</div>

<p>
Then reformulate it a little bit: 
</p>
<div class="org-center">
<p>
\(log(P(\X|\pa))= \sum_{\Z}q(\Z) log(\frac{P(\X,\Z|\pa)}{q(\Z)}) - \sum_{\Z}q(\Z) log(\frac{P(\Z|\X,\pa)}{q(\Z)})\) 
</p>
</div>

<p>
Two terms appear. The second one is the Kullback-Leibler divergence between \(P(\Z|\X,\pa)\) and \(q(\Z)\) and the first one is denoted \(\lb\). 
</p>
<div class="org-center">
<p>
\(log(P(\X|\pa))=  \lb + \dkl(q(\Z)||P(\Z|\X,\pa))\)
</p>
</div>

<p>
Recall that the Kullback-Leibler divergence satisfies \(\dkl(q|p)\ge 0\), with equality if, and only if \(q(Z) = p(Z|X,\pa)\). Therefore \(P(\X|\pa) \ge \lb\) which means that \(\lb\) is a lower bound of \(P(\X|\pa)\) we want to maximize. This formulation does not solve the tractability issue since in the \(\dkl\) term we still have a dependance between \(P(\Z|X,\pa)\) and \(P(\X|\pa)\), <i>i.e</i> :
</p>
<div class="org-center">
<p>
\(P(\Z|X,\pa) = \frac{P(\X,\Z|\pa)}{P(\X|\pa)}\).
</p>
</div>
<p>
So both \(P(\Z|X,\pa)\) and \(P(\X|\pa)\) are untractable. The probabilistic model specifies the joint distribution \(P(\X, \Z)\), and the goal is to find an approximation for the posterior distribution \(P(\Z|\X,\pa)\) as well as for the model evidence \(P(X|\pa)\). 
</p>

<p>
To quote Christopher Bishop (see chapter ten of <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a>): 
</p>
<blockquote>
<p>
Although there is nothing intrinsically approximate about variational methods, they do naturally lend themselves to finding approximate solutions. This is done by restricting the range of functions over which the optimization is performed, for instance by considering only quadratic functions or by considering functions com- posed of a linear combination of fixed basis functions in which only the coefficients of the linear combination can vary. In the case of applications to probabilistic in- ference, the restriction may for example take the form of factorization assumptions
</p>
</blockquote>
</div>
</div>


<div id="outline-container-orgd9deebe" class="outline-2">
<h2 id="orgd9deebe"><span class="section-number-2">2</span> The EM view</h2>
<div class="outline-text-2" id="text-2">
<p>
The EM algorithm aims at estimating the parameters \(\pa\) of a generative model that relies on latent variables \(\Z\) to explain the observed data \(\X\). In the example of mixture of gaussians, \(\Z\) represents the affectation of each observation to a gaussian. If \(\Z\) could be observed, it becomes therefore a <i>simple</i> and easy to solve classification problem. We can call \((\X, \Z)\) the complete data set, while  \((\X)\) is the  incomplete one.  In other words, \(P(\X,\Z|\pa)\) is easy to optimize, while \(P(\X|\pa)\) is untractable (a logsum). 
</p>

<p>
However, in practice \(\Z\) is unknown, but for a given set of parameters we can compute \(P(\Z|\X,\pa)\) and also the <b>expected</b> value of \(\Z|\X,\pa\). This is the <b>E(xpectation) step</b>. In a second step, the classification task can be carried out: <b>maximizing</b> \(\pa\) knowing \(\Z\) (or the expected value). This is the <b>M(aximization) step</b>. 
</p>

<p>
Now we can go back to the lower bound to explain the EM algorithm. This is a two-stage iterative optimization technique for finding maximum likelihood solutions.
</p>
</div>

<div id="outline-container-orgefdcc8d" class="outline-3">
<h3 id="orgefdcc8d"><span class="section-number-3">2.1</span> E step</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Suppose that the current value of the parameter vector is \(\paold\). In the E step, the lower bound \(\lb\) <b>is maximized with respect to \(q(\Z)\)</b> while holding \(\pa\) fixed to \(\paold\). If you remember that: 
</p>
<ul class="org-ul">
<li>\(P(\X,\Z|\paold)=P(\Z|\X,\paold)P(\X|\paold)\),</li>
<li>so the solution is when \(q(\Z) = P(\Z|\X,\paold)\).</li>
<li>Therefore, the Kullback-Leibler divergence vanishes.</li>
</ul>
<p>
Since the KL divergence is zero, we have \(\lb=P(\X|\paold)\). In fact, the E-step consists in computing the posterior distribution over \(\Z\) with the parameters fixed at \(\paold\). Then you just set <i>theoritically</i> \(q(\Z) = P(\Z|\X,\paold)\). 
</p>
</div>
</div>

<div id="outline-container-org7d7fbe8" class="outline-3">
<h3 id="org7d7fbe8"><span class="section-number-3">2.2</span> M step</h3>
<div class="outline-text-3" id="text-2-2">
<p>
The distribution \(q(Z)\) is now fixed and the <b>lower bound \(\lb\) is now maximized with respect to \(\pa\)</b>.  The maximization process yields a new value for the parameters \(\panew\) and increases the lower bound (except if we are already at the maximum). Note that \(q(Z)=P(\Z|\X,\paold)\) acts as a constant in the maximization process: 
</p>

<div class="org-center">
<p>
\(\lb =  \sum_{\Z} P(\Z|\X,\paold) log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\paold)}) = \sum_{\Z} P(\Z|\X,\paold) log(P(\X,\Z|\pa)) - \sum_{\Z} P(\Z|\X,\paold) log(P(\Z|\X,\paold))\)
</p>
</div>
<p>
The second term is a positive constant, since it depends only on \(\paold\). This is the entropy of the posterior distribution \(H(P(\Z|\X,\paold))\). The first we want to maximize is the expectation under the posterior \(P(\Z|\X,\paold)\) of the log-likelihood of complete data. This means in practice, that we optimize a classifier of \(\X\) into \(\Z\), with the supervision of  \(P(\Z|\X,\paold)\) that provides the pseudo-affectation. 
</p>


<p>
Since the distribution \(q\) is fixed to \(P(\Z|\X,\paold)\), \(q(Z) \neq P(\Z|\X,\panew)\) and now the KL divergence term is nonzero. The increase in the log likelihood function is therefore greater than the increase in the lower bound. 
</p>
</div>
</div>

<div id="outline-container-org163e935" class="outline-3">
<h3 id="org163e935"><span class="section-number-3">2.3</span> A summary of EM</h3>
<div class="outline-text-3" id="text-2-3">
<p>
You can see the EM algorith as pushing two functions. 
</p>
<ul class="org-ul">
<li>With fixed parameters (\(\paold\)), push \(\lb\) to stick \(P(\X|\paold)\) by setting \(q(Z)=P(\Z|\X,\paold)\)</li>
<li>Then recompute the parameters to get \(\panew\) with a fixed \(q(\Z)=P(\Z|\X,\paold)\). The criterion is to maximize \(\lb\) w.r.t \(\pa\). In fact you are moving in the parameter space in order to push \(\lb\), but by doing this you also push the likelihood even further: \(P(\X|\panew) \ge \lb\) because \(P(\Z|\X,\pa)\) has changed to  \(P(\Z|\X,\panew)\) while \(q(\Z)\) has not.</li>
</ul>

<p>
For graphical illustration, you can look at the book of Christopher Bishop  called <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a>. You can also read the great paper of Radford Neal and Geoffrey Hinton on the link between <a href="ftp://ftp.cdf.toronto.edu/dist/radford/emk.pdf">Variational Bayes and EM</a>.
</p>

<p>
Note that for implementation, \(q(\Z)\) does not really exist or is not a quantity of interest. It acts more like a temporary variable to perform this two steps game. 
</p>
</div>
</div>
</div>
