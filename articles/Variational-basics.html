---
title: "Variational Bayes : some basics"
date: 2017-07-21
layout: post
categories: 
tags: 
- article 
- bayes 
- variation 
- EM
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgd9deebe">1. The EM view</a>
<ul>
<li><a href="#orgefdcc8d">1.1. E step</a></li>
<li><a href="#org7d7fbe8">1.2. M step</a></li>
<li><a href="#org8267418">1.3. A summary</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>
<p>
\(\newcommand{\X}{\mathbf{X}}\)
\(\newcommand{\Z}{\mathbf{Z}}\)
\(\newcommand{\pa}{\mathbf{\theta}}\)
\(\newcommand{\paold}{\pa^{old}}\)
\(\newcommand{\panew}{\pa^{new}}\)
</p>

<p>
\(\newcommand{\lb}{\mathcal{L}(q;\pa)}\) 
</p>

<p>
\(\newcommand{\dkl}{D_{kl}}\)
</p>


<p>
Consider a probabilistic model in which we denote all of the observed variables by \(\X\) and all and all of the hidden variables by \(\Z\). The joint distribution \(P(\X, \Z|\pa)\) is defined by a set of parameters denoted \(\pa\). The goal is to maximize the likelihood function \(P(\X|\pa)\) to learn the parameters \(\pa\). The problem is that this maximization problem is in most of the case intractable.  This likelihood can be expressed in two ways: \(P(\X|\pa)= \sum_{\Z} P(\X,\Z|\pa)= \frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)}\). Let us consider the second one in this log formulation:
</p>

<div class="org-center">
<p>
\(log(P(\X|\pa)= log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)})\)
</p>
</div>

<p>
Next we introduce a distribution \(q(\Z)\) defined over the latent variables by multliplying both parts, and we can also sum over \(\Z\) 
</p>
<div class="org-center">
<p>
\(\sum_{\Z} q(\Z) log(P(\X|\pa)= \sum_{\Z}log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)})q(\Z)\)
</p>
</div>

<p>
The right hand side is just \(P(\X|\pa)\) and remains unchanged. The second trick is to introduce \(q(\Z)\) in the fraction:
</p>
<div class="org-center">
<p>
\(log(P(\X|\pa))= \sum_{\Z}q(\Z) log(\frac{P(\X,\Z|\pa)q(\Z)}{P(\Z|\X,\pa)q(\Z)})\)
</p>
</div>

<p>
Then reformulate it a little bit: 
</p>
<div class="org-center">
<p>
\(log(P(\X|\pa))= \sum_{\Z}q(\Z) log(\frac{P(\X,\Z|\pa)}{q(\Z)}) - \sum_{\Z}q(\Z) log(\frac{P(\Z|\X,\pa)}{q(\Z)})\) 
</p>
</div>

<p>
Two terms appear. The second one is the Kullback-Leibler divergence between \(P(\Z|\X,\pa)\) and \(q(\Z)\) and the first one is denoted \(\lb\). 
</p>
<div class="org-center">
<p>
\(log(P(\X|\pa))=  \lb + \dkl(q(\Z)||P(\Z|\X,\pa))\)
</p>
</div>

<p>
Recall that the Kullback-Leibler divergence satisfies \(\dkl(q|p)\ge 0\), with equality if, and only if \(q(Z) = p(Z|X,\pa)\). Therefore \(P(\X|\pa) \ge \lb\) which means that \(\lb\) is a lower bound of \(P(\X|\pa)\) we want to maximize.
</p>

<div id="outline-container-orgd9deebe" class="outline-2">
<h2 id="orgd9deebe"><span class="section-number-2">1</span> The EM view</h2>
<div class="outline-text-2" id="text-1">
<p>
The EM algorithm aims at estimating the parameters \(\pa\) of a generative model that relies on latent variables \(\Z\) to explain the observed data \(\X\). In the example of mixture of gaussians, \(\Z\) represents the affectation of each observation to a gaussian. If \(\Z\) could be observed, it becomes therefore a <i>simple</i> and easy to solve classification problem. We can call \((\X, \Z)\) the complete data set, while  \((\X)\) is the  incomplete one.  In other words, \(P(\X,\Z|\pa)\) is easy to optimize, while \(P(\X|\pa)\) is untractable (a logsum). 
</p>

<p>
However, in practice \(\Z\) is unknown, but for a given set of parameters we can compute \(P(\Z|\X,\pa)\) and also the <b>expected</b> value of \(\Z|\X,\pa\). This is the <b>E(xpectation) step</b>. In a second step, the classification task can be carried out: <b>maximizing</b> \(\pa\) knowing \(\Z\) (or the expected value). This is the <b>M(aximization) step</b>. 
</p>

<p>
Now we can go back to the lower bound to explain the EM algorithm. This is a two-stage iterative optimization technique for finding maximum likelihood solutions.
</p>
</div>

<div id="outline-container-orgefdcc8d" class="outline-3">
<h3 id="orgefdcc8d"><span class="section-number-3">1.1</span> E step</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Suppose that the current value of the parameter vector is \(\paold\). In the E step, the lower bound \(\lb\) <b>is maximized with respect to \(q(\Z)\)</b> while holding \(\pa\) fixed to \(\paold\). If you remember that: 
</p>
<ul class="org-ul">
<li>\(P(\X,\Z|\paold)=P(\Z|\X,\paold)P(\X|\paold)\),</li>
<li>so the solution is when \(q(\Z) = P(\Z|\X,\paold)\).</li>
<li>Therefore, the Kullback-Leibler divergence vanishes.</li>
</ul>
<p>
Since the KL divergence is zero, we have \(\lb=P(\X|\paold)\). In fact, the E-step consists in computing the posterior distribution over \(\Z\) with the parameters fixed at \(\paold\). Then you just set <i>theoritically</i> \(q(\Z) = P(\Z|\X,\paold)\). 
</p>
</div>
</div>

<div id="outline-container-org7d7fbe8" class="outline-3">
<h3 id="org7d7fbe8"><span class="section-number-3">1.2</span> M step</h3>
<div class="outline-text-3" id="text-1-2">
<p>
The distribution \(q(Z)\) is now fixed and the <b>lower bound \(\lb\) is now maximized with respect to \(\pa\)</b>.  The maximization process yields a new value for the parameters \(\panew\) and increases the lower bound (except if we are already at the maximum). Note that \(q(Z)=P(\Z|\X,\paold)\) acts as a constant in the maximization process: 
</p>
<ul class="org-ul">
<li>$\lb =  &sum;<sub>\Z</sub> P(\Z|\X,\paold) log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\paold)}) = &sum;<sub>\Z</sub> P(\Z|\X,\paold) log(P(\X,\Z|\pa)) - &sum;<sub>\Z</sub> P(\Z|\X,\paold) log(P(\Z|\X,\paold)) $</li>
</ul>



<p>
Since the distribution \(q\) is fixed to \(P(\Z|\X,\paold)\), \(q(Z) \neq P(\Z|\X,\panew)\) and now the KL divergence term is nonzero. The increase in the log likelihood function is therefore greater than the increase in the lower bound. 
</p>
</div>
</div>

<div id="outline-container-org8267418" class="outline-3">
<h3 id="org8267418"><span class="section-number-3">1.3</span> A summary</h3>
<div class="outline-text-3" id="text-1-3">
<p>
You can see the EM algorith as pushing two functions. 
</p>
<ul class="org-ul">
<li>With fixed parameters (\(\paold\)), push \(\lb\) to stick \(P(\X|\paold)\) by setting \(q(Z)=P(\Z|\X,\paold)\)</li>
<li>Then recompute the parameters to get \(\panew\) with a fixed \(q(\Z)=P(\Z|\X,\paold)\). The criterion is to maximize \(\lb\) w.r.t \(\pa\). In fact you are moving in the parameter space in order to push \(\lb\), but by doing this you also push the likelihood even further: \(P(\X|\panew) \ge \lb\) because \(P(\Z|\X,\pa)\) has changed to  \(P(\Z|\X,\panew)\) while \(q(\Z)\) has not.</li>
</ul>

<p>
For graphical illustration, you look at the book of Christopher Bishop  called <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a>. 
</p>

<p>
Note that for implementation, \(q(\Z)\) does not really exist or is not a quantity of interest. It acts more like a temporary variable to perform this two steps game. 
</p>
</div>
</div>
</div>
