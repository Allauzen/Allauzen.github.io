---
title: "Variational Bayes / some basics"
date: 2017-07-21
layout: post
categories: 
tags: 
- article 
- bayes 
- variational 
- EM
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org304f6e1">1. Variational Bayes: Bayesian "approximate (or not)" inference</a>
<ul>
<li><a href="#orgdc611e5">1.1. Variational derivation</a></li>
<li><a href="#orgae0dc76">1.2. Approximate inference</a></li>
</ul>
</li>
<li><a href="#org7abd66f">2. The EM view</a>
<ul>
<li><a href="#org836513d">2.1. E step</a></li>
<li><a href="#org95f90b0">2.2. M step</a></li>
<li><a href="#org15bb494">2.3. A summary of EM</a></li>
</ul>
</li>
<li><a href="#orgdb22905">3. The problem of approximate inference in general</a>
<ul>
<li><a href="#org902f388">3.1. The evidence lower bound (ELBO)</a></li>
<li><a href="#org09bd4ea">3.2. ELBO : some comments</a></li>
<li><a href="#org0a52bfc">3.3. <span class="todo TODO">TODO</span> Example of variational inference</a></li>
</ul>
</li>
<li><a href="#org859a068">4. Variational inference or MCMC</a></li>
</ul>
</div>
</div>
<hr>

<p>
\(\newcommand{\X}{\mathbf{X}}\)
\(\newcommand{\Z}{\mathbf{Z}}\)
\(\newcommand{\pa}{\mathbf{\theta}}\)
\(\newcommand{\paold}{\pa^{old}}\)
\(\newcommand{\panew}{\pa^{new}}\)
\(\newcommand{\lb}{\mathcal{L}(q;\pa)}\) 
</p>

<p>
\(\newcommand{\dkl}{D_{kl}}\)
\(\newcommand{\elbo}{\textrm{elbo}}\)
</p>



<p>
Consider a probabilistic model in which we denote all the observed variables by \(\X\) and all the hidden variables by \(\Z\). In general, the probabilistic model is designed by a generative story which implies a definition of the joint probability \(P(\X, \Z|\pa)\) or \(P(\X, \Z,\pa)\) in the real bayesian view.
</p>

<p>
If we consider the joint distribution \(P(\X, \Z|\pa)\), it is defined by a set of parameters denoted \(\pa\). The goal is to maximize the likelihood function \(P(\X|\pa)\) to learn the parameters \(\pa\). The problem is that this maximization problem is in most of the case intractable.
</p>

<p>
If we consider \(P(\X, \Z,\pa)\), there are different levels of bayesian inference: 
</p>
<ul class="org-ul">
<li>infer  \(P(\pa|\X)\) which also implies a summation over \(\Z\)</li>
<li>infer \(P(\X)\) for prediction purpose</li>
<li>infer \(P(\Z|\X,\pa)\), the posterior distribution of the latent variable to better analyse new data.</li>
</ul>

<p>
In all cases, the inference is untractable because we need to estimate \(P(\X|\pa)\) or even marginalize parameters. This requires approximate inference approach, such as MCMC or Variational one.
</p>

<div id="outline-container-org304f6e1" class="outline-2">
<h2 id="org304f6e1"><span class="section-number-2">1</span> Variational Bayes: Bayesian "approximate (or not)" inference</h2>
<div class="outline-text-2" id="text-1">
<p>
This likelihood can be expressed in two ways: 
\[P(\X|\pa)= \sum_{\Z} P(\X,\Z|\pa)= \frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)}\]. 
</p>

<p>
The "untractability" comes from marginalization: in the first case, sum over \(\Z\); while in the second case, the same issue is hidden in the latent posterior distribution \(P(\Z|\X,\pa)\). 
Let us consider the second one in its log formulation:
</p>
<div class="org-center">
<p>
\(\log(P(\X|\pa))= \log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)})\)
</p>
</div>
</div>
<div id="outline-container-orgdc611e5" class="outline-3">
<h3 id="orgdc611e5"><span class="section-number-3">1.1</span> Variational derivation</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Let introduce a distribution \(q(\Z)\) defined over the latent variables. This probability function is an approximate of the true posterior \(P(\Z|\X,\pa)\). <b>The goal is to find the distribution \(q\) that best approximates the posterior distribution and then to use it as a proxy for Bayesian inference</b>. 
</p>

<p>
To introduce \(q\), we can multliply both parts of the previous equation with \(q\), and sum  over \(\Z\): 
</p>
<div class="org-center">
<p>
\(\sum_{\Z} q(\Z) \log(P(\X|\pa))= \sum_{\Z}\log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\pa)})q(\Z)\)
</p>
</div>

<p>
The right hand side is just \(P(\X|\pa)\) and remains unchanged. The second trick is to introduce \(q(\Z)\) in the fraction:
</p>
<div class="org-center">
<p>
\(\log(P(\X|\pa))= \sum_{\Z}q(\Z) \log(\frac{P(\X,\Z|\pa)q(\Z)}{P(\Z|\X,\pa)q(\Z)})\) 
</p>
</div>

<p>
Then reformulate it a little bit: 
</p>
<div class="org-center">
<p>
\(\log(P(\X|\pa))= \sum_{\Z}q(\Z) \log(\frac{P(\X,\Z|\pa)}{q(\Z)}) + \sum_{\Z}q(\Z) \log(\frac {q(\Z)}{P(\Z|\X,\pa)})\) 
</p>
</div>

<p>
Two terms appear. The second one is the Kullback-Leibler divergence between  \(q(\Z)\) and \(P(\Z|\X,\pa)\): 
\[
\dkl(q(\Z)||P(\Z|\X,\pa)) = \sum_{\Z} q(\Z) \log(\frac{q(\Z)}{P(\Z|\X)}).
\]
And the first term is for the moment denoted by \(\lb\). We then obtain the following relation: 
</p>
\begin{align}
\log(P(\X|\pa)) &=  \lb + \dkl(q(\Z)||P(\Z|\X,\pa))\\
\lb &= \sum_{\Z}q(\Z) \log(\frac{P(\X,\Z|\pa)}{q(\Z)}) \\
&= E_{q}[ \log(\frac{P(\X,\Z|\pa)}{q(\Z)}) ] \\
&=  E_{q}[ \log(P(\X|\Z, \pa) ] - \dkl(q(\Z)| P(\Z))
\end{align}

<p>
Recall that the Kullback-Leibler divergence satisfies \(\dkl(q|p)\ge 0\), with equality if, and only if \(q(\Z) = p(\Z|\X,\pa)\). Therefore \(P(\X|\pa) \ge \lb\) which means that \(\lb\) is a lower bound of \(P(\X|\pa)\) we want to maximize.  \(\lb\) is therefore named the <b>Evidence Lower Bound or \(\elbo\)</b>. 
</p>



<p>
This formulation does not solve the tractability issue since in the \(\dkl\) term we still have a dependance between \(P(\Z|X,\pa)\) and \(P(\X|\pa)\), <i>i.e</i> :
</p>
<div class="org-center">
<p>
\(P(\Z|X,\pa) = \frac{P(\X,\Z|\pa)}{P(\X|\pa)}\).
</p>
</div>
<p>
So both \(P(\Z|X,\pa)\) and \(P(\X|\pa)\) are untractable. The probabilistic model specifies the joint distribution \(P(\X, \Z)\), and the goal is to find an approximation for the posterior distribution \(P(\Z|\X,\pa)\) as well as for the model evidence \(P(X|\pa)\). 
</p>

<p>
To quote Christopher Bishop (see chapter ten of <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a>): 
</p>
<blockquote>
<p>
Although there is nothing intrinsically approximate about variational methods, they do naturally lend themselves to finding approximate solutions. This is done by restricting the range of functions over which the optimization is performed, for instance by considering only quadratic functions or by considering functions composed of a linear combination of fixed basis functions in which only the coefficients of the linear combination can vary. In the case of applications to probabilistic inference, the restriction may for example take the form of factorization assumptions
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgae0dc76" class="outline-3">
<h3 id="orgae0dc76"><span class="section-number-3">1.2</span> Approximate inference</h3>
<div class="outline-text-3" id="text-1-2">
<p>
To solve the tractability issue, there are two common approaches: 
</p>
<ul class="org-ul">
<li>The E.M algorithm that fix the parameters during the E step and then compute when it's possible \(P(\Z|\X;\pa)\) to then optimize the \(\elbo\).</li>

<li>Otherwise we can directly optimize the \(\elbo\) term. This can be done with some further assumptions on \(q\) like mean-fields.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org7abd66f" class="outline-2">
<h2 id="org7abd66f"><span class="section-number-2">2</span> The EM view</h2>
<div class="outline-text-2" id="text-2">
<p>
The EM algorithm aims at estimating the parameters \(\pa\) of a generative model that relies on latent variables \(\Z\) to explain the observed data \(\X\). In the example of mixture of gaussians, \(\Z\) represents the affectation of each observation to a gaussian. If \(\Z\) could be observed, it becomes therefore a <i>simple</i> and easy to solve classification problem. We can call \((\X, \Z)\) the complete data set, while  \((\X)\) is the  incomplete one.  In other words, \(P(\X,\Z|\pa)\) is easy to optimize, while \(P(\X|\pa)\) is untractable (the log-sum). 
</p>

<p>
However, in practice \(\Z\) is unknown, but for a given set of parameters we can compute \(P(\Z|\X,\pa)\) and also the <b>expected</b> value of \(\Z|\X,\pa\). This is the <b>E(xpectation) step</b>. In a second step, the classification task can be carried out: <b>maximizing</b> \(\pa\) knowing \(\Z\) (or the expected value). This is the <b>M(aximization) step</b>. 
</p>

<p>
Now we can go back to the lower bound to explain the EM algorithm. This is a two-stage iterative optimization technique for finding maximum likelihood solutions.
</p>
</div>

<div id="outline-container-org836513d" class="outline-3">
<h3 id="org836513d"><span class="section-number-3">2.1</span> E step</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Suppose that the current value of the parameter vector is \(\paold\). In the E step, the lower bound \(\lb\) <b>is maximized with respect to \(q(\Z)\)</b> while holding \(\pa\) fixed to \(\paold\). If you remember that: 
</p>
<ul class="org-ul">
<li>\(P(\X,\Z|\paold)=P(\Z|\X,\paold)P(\X|\paold)\),</li>
<li>so the solution is when \(q(\Z) = P(\Z|\X,\paold)\).</li>
<li>Therefore, the Kullback-Leibler divergence vanishes.</li>
</ul>
<p>
Since the KL divergence is zero, we have \(\lb=P(\X|\paold)\). In fact, the E-step consists in computing the posterior distribution over \(\Z\) with the parameters fixed at \(\paold\). Then you just set <i>theoritically</i> \(q(\Z) = P(\Z|\X,\paold)\). 
</p>
</div>
</div>

<div id="outline-container-org95f90b0" class="outline-3">
<h3 id="org95f90b0"><span class="section-number-3">2.2</span> M step</h3>
<div class="outline-text-3" id="text-2-2">
<p>
The distribution \(q(Z)\) is now fixed and the <b>lower bound \(\lb\) is now maximized with respect to \(\pa\)</b>.  The maximization process yields a new value for the parameters \(\panew\) and increases the lower bound (except if we are already at the maximum). Note that \(q(Z)=P(\Z|\X,\paold)\) acts as a constant in the maximization process: 
</p>

\begin{align}
\lb &=  \sum_{\Z} P(\Z|\X,\paold) \log(\frac{P(\X,\Z|\pa)}{P(\Z|\X,\paold)}) \\
&= \sum_{\Z} P(\Z|\X,\paold) \log(P(\X,\Z|\pa)) - \sum_{\Z} P(\Z|\X,\paold) \log(P(\Z|\X,\paold))
\end{align}
<p>
The second term is a positive constant, since it depends only on \(\paold\). This is the entropy of the posterior distribution \(H(P(\Z|\X,\paold))\). The first we want to maximize is the expectation under the posterior \(P(\Z|\X,\paold)\) of the log-likelihood of complete data. This means in practice, that we optimize a classifier of \(\X\) into \(\Z\), with the supervision of  \(P(\Z|\X,\paold)\) that provides the pseudo-affectation. 
</p>


<p>
Since the distribution \(q\) is fixed to \(P(\Z|\X,\paold)\), \(q(Z) \neq P(\Z|\X,\panew)\) and now the KL divergence term is nonzero. The increase in the log likelihood function is therefore greater than the increase in the lower bound. 
</p>
</div>
</div>

<div id="outline-container-org15bb494" class="outline-3">
<h3 id="org15bb494"><span class="section-number-3">2.3</span> A summary of EM</h3>
<div class="outline-text-3" id="text-2-3">
<p>
You can see the EM algorithm as pushing two functions. 
</p>
<ul class="org-ul">
<li>With fixed parameters (\(\paold\)), push \(\lb\) to stick \(P(\X|\paold)\) by setting \(q(Z)=P(\Z|\X,\paold)\)</li>
<li>Then recompute the parameters to get \(\panew\) with a fixed \(q(\Z)=P(\Z|\X,\paold)\). The criterion is to maximize \(\lb\) w.r.t \(\pa\). In fact you are moving in the parameter space in order to push \(\lb\), but by doing this you also push the likelihood even further: \(P(\X|\panew) \ge \lb\) because \(P(\Z|\X,\pa)\) has changed to  \(P(\Z|\X,\panew)\) while \(q(\Z)\) has not.</li>
</ul>

<p>
For graphical illustration, you can look at the book of Christopher Bishop  called <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a>. You can also read the great paper of Radford Neal and Geoffrey Hinton on the link between <a href="ftp://ftp.cdf.toronto.edu/dist/radford/emk.pdf">Variational Bayes and EM</a>.
</p>

<p>
Note that for implementation, \(q(\Z)\) does not really exist or is not a quantity of interest. It acts more like a temporary variable to perform this two steps game. 
</p>
</div>
</div>
</div>



<div id="outline-container-orgdb22905" class="outline-2">
<h2 id="orgdb22905"><span class="section-number-2">3</span> The problem of approximate inference in general</h2>
<div class="outline-text-2" id="text-3">
<p>
In this part, there is no mention of hyperparameters or even parameters. Let say that the latent variables represent all unknown quantities of interest.
</p>

<p>
A generative model defines a joint density of latent variables \(\Z\) and observations \(\X\), \[p(\Z, \X) = p(Z)p(\X | \Z),\] with a generative scenario: draw the latent variables from a prior density \(p(\Z)\) and then relates them to the observations through the likelihood $p(\X|\Z). Inference in a Bayesian model amounts to conditioning on data and computing the posterior $p(\Z | \X). In fact, the latent variables \(\Z\) help govern the distribution of the data.
</p>

<p>
To estimate \(p(\Z|\X)\), the Bayes formula tells us: 
</p>

<p>
\[p(\Z|\X) = \frac{p(\X,\Z)}{p(\X)}\] 
</p>

<p>
This is not of a great help since the denominator is the evidence and is also untractable: the summation over all the possible values of \(\Z\).
</p>
</div>
<div id="outline-container-org902f388" class="outline-3">
<h3 id="org902f388"><span class="section-number-3">3.1</span> The evidence lower bound (ELBO)</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Given a family of distribution, we are looking for one its member \(q(\Z)\) that best approximates the conditional distribution \(p(Z|X)\). The criterion is the Kullback-Leibler divergence :
</p>
<div class="org-center">
<p>
\(q^*(\Z) = argmin_{q} \dkl(q(\Z)||p(\Z|\X))\) 
</p>
</div>
<p>
This objective is not tractable since its relies on \(p(\Z|\X)\) and hence on \(p(\X)\) : 
</p>
\begin{align}
\dkl(q(\Z)||p(\Z|\X)) &= E_{q}[\log q(\Z)] - E_{q}[\log p(\Z|\X)]\\
&=  E_{q}[\log q(\Z)] - E_{q}[\log p(\X,\Z)] + E_{q}[\log p(\X)]\\
&=  E_{q}[\log q(\Z)] - E_{q}[\log p(\X,Z)] + \log p(\X)\\
\end{align}
<p>
This shows the dependence of the objective on the data evidence, but also that \[\dkl(q(\Z)||p(\Z|\X)) \ge E_{q}[\log q(\Z)] - E_{q}[\log p(\X,\Z)].\] Minimizing the second term is equivalent to minimize the KL divergence objective.  The term \(\log p(\X)\) is independent of \(q(\Z)\) and thus a negative constant.
</p>

<p>
The quantity \(E_{q}[\log p(\X,\Z)]- E_{q}[\log q(\Z)]\) is called the <b>evidence lower bound (ELBO)</b>.  The term ELBO comes from the fact that if you reorganise the equation, you get: 
</p>

\begin{align}
\log p(\X) &= \dkl(q(\Z)||p(\Z|\X))+ E_{q}[\log p(\X,\Z)]- E_{q}[\log q(\Z)] \\
&= \dkl(q(\Z)||p(\Z|\X)) + \elbo(q) \\
&\ge \elbo(q)
\end{align}

<p>
Therefore maximizing the elbo term implies both to find the best approximate \(q(\Z)\) within a family of distribution of \(p(\Z|\X)\) and also to maximize a lower bound of \(p(\X)\).
</p>

<p>
For further insight, the elbo can be rewritten as: 
</p>

\begin{align}
\elbo(q)&= E_{q}[\log p(\Z)] + E_q [\log p(\X|\Z)]- E_q [\log q(\Z)]\\
&=  E_q [\log p(\X|\Z)] - \dkl(q(\Z)||p(\Z))
\end{align}

<p>
The first term is an expected log likelihood under the \(q\) distribution and the second is the divergence between the variational density and the prior. Therefore maximizing \(\elbo(q)\) resorts to maximizing the expected log likelihood under the \(q\) distribution, while keeping \(q\) as close as possible to the prior \(p(\Z)\). 
</p>

<p>
In a lot of papers and books on variational inference, the elbo is  derived through Jensen's inequality. See the <a href="https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf">Jordan's paper in 1999</a> for instance. 
</p>
</div>
</div>

<div id="outline-container-org09bd4ea" class="outline-3">
<h3 id="org09bd4ea"><span class="section-number-3">3.2</span> ELBO : some comments</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Variational inference aims at maximizing the ELBO. This means that we indirectly optimize two terms: 
</p>
<ul class="org-ul">
<li>maximizing \(p(\X)\), the data evidence, while</li>
<li>minimizing the divergence \(\dkl(q(\Z)||p(\Z|\X))\).</li>
</ul>
<p>
The elbo term can written as: 
</p>
\begin{align} 
\lb &= E_{q}[log p(\X,\Z|\pa)]- E_{q}[log q(\Z)] .
\end{align} 
<p>
Maximizing the ELBO, means thus to jointly  maximize \(E_{q}[log p(\X,\Z|\pa)]\) and the entropy of the distribution \(q\). However we can further decompose the joint distribution: 
</p>
\begin{align} 
\lb &= E_{q}[log p(\X|,\Z\pa) p(\Z|\pa)]- E_{q}[log q(\Z)] \\
&= E_{q}[log p(\X|,\Z\pa)] + E_{q}[p(\Z|\pa)-log q(\Z)] \\
&= E_{q}[log p(\X|,\Z\pa)] - \dkl( q(\Z)||p(\Z|\pa) ).
\end{align} 
<p>
Therefore the second term can be interpreted as a r√©gularization term on \(q\). 
</p>
</div>
</div>


<div id="outline-container-org0a52bfc" class="outline-3">
<h3 id="org0a52bfc"><span class="section-number-3">3.3</span> <span class="todo TODO">TODO</span> Example of variational inference</h3>
</div>
</div>

<div id="outline-container-org859a068" class="outline-2">
<h2 id="org859a068"><span class="section-number-2">4</span> Variational inference or MCMC</h2>
<div class="outline-text-2" id="text-4">
<p>
Here is quoted, what you can read in this <a href="https://arxiv.org/abs/1601.00670">review from Blei et al.</a>, comparing variational inference and MCMC:
</p>


<blockquote>
<p>
When should a statistician use MCMC and when should she use variational inference? We will offer some guidance. MCMC methods tend to be more computationally intensive than variational inference but they also provide guarantees of producing (asymptotically) exact samples from the target density (Robert and Casella, 2004). Variational inference does not enjoy such guarantees it can only find a density close to the target but tends to be faster than MCMC. Because it rests on optimization, variational inference easily takes advantage of methods like stochastic opti- mization (Robbins and Monro, 1951; Kushner and Yin, 1997) and distributed optimization (though some MCMC methods can also exploit these innovations (Welling and Teh, 2011; Ahmed et al., 2012)).  
</p>

<p>
Thus, variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we happily pay a heavier computational cost for more precise samples. For example, we might use MCMC in a setting where we spent 20 years collecting a small but expensive data set, where we are confident that our model is appropriate, and where we require precise inferences. We might use variational inference when fitting a probabilistic model of text to one billion text documents and where the inferences will be used to serve search results to a large population of users. In this scenario, we can use distributed computation and stochastic optimization to scale and speed up inference, and we can easily explore many different models of the data.
</p>

<p>
Data set size is not the only consideration. Another factor is the geometry of the posterior distribution. For example, the posterior of a mixture model admits multiple modes, each corresponding label permutations of the components. Gibbs sampling, if the model permits, is a powerful approach to sampling from such target distributions; it quickly focuses on one of the modes. For mixture models where Gibbs sampling is not an option, variational inference may perform better than a more general MCMC technique (e.g., Hamiltonian Monte Carlo), even for small datasets (Kucukelbir et al., 2015). Exploring the interplay between model complexity and inference (and between variational inference and MCMC) is an exciting avenue for future research (see Section 5.4).
(&#x2026;)
</p>
</blockquote>
</div>
</div>
