<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-06-24 lun. 17:13 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Rationalizing Neural Predictions</title>
<meta name="author" content="A. Allauzen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'left',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'left',
      displayIndent: '5em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'left',
      displayIndent: '5em'
    },
    output: {
      font: 'mathjax-euler',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Rationalizing Neural Predictions</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org97f8b6c">Summary</a>
<ul>
<li><a href="#org78fac83">Generator</a></li>
<li><a href="#org205f37d">Encoder</a></li>
<li><a href="#org282dc88">Joint optimization</a></li>
<li><a href="#orgdca14e0">Inference</a></li>
</ul>
</li>
<li><a href="#org5372019">Questions / Comments</a>
<ul>
<li><a href="#orgeb52df1">Rationale</a></li>
<li><a href="#org729b6aa">Training and inference</a></li>
<li><a href="#org48235f2">Attention based model for classification</a></li>
<li><a href="#orgf9cfa9b">Encoder</a></li>
<li><a href="#orgea3338d">Loss function etc &#x2026;</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
<div style="display: none">
\(
\global\def\normal{\mathcal{N}}
\global\def\m{\mu}
\global\def\dkl{D_{kl}}
\global\def\X{\mathbf{X}}
\global\def\Y{\mathbf{Y}}
\global\def\Z{\mathbf{Z}}
\global\def\x{\mathbf{x}}
\global\def\y{\mathbf{y}}
\global\def\z{\mathbf{z}}
\global\def\M{\boldsymbol{\mu}}
\global\def\C{\boldsymbol{\Sigma}}
\global\def\pis{\boldsymbol{\Pi}}
\global\def\pa{\boldsymbol{\theta}}
\global\def\paold{\pa^{old}}
\global\def\panew{\pa^{new}}
\global\newcommand\lb{\mathcal{L}(q;\pa)}
\global\newcommand\elbo{\textrm{elbo}}
\global\newcommand\lsrc{I}
\global\newcommand\ltrg{J}
\)
</div>
</p>
<hr>



<p>
Reading notes on "Rationalizing Neural Predictions", by Tao Lei,
Regina Barzilay and Tommi Jaakkola, published at EMNLP 2016. You can
download <a href="https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf">the paper</a> and <a href="https://people.csail.mit.edu/taolei/papers/emnlp16_rationale_slides.pdf">the slides</a>.
</p>




<div id="outline-container-org97f8b6c" class="outline-2">
<h2 id="org97f8b6c">Summary</h2>
<div class="outline-text-2" id="text-org97f8b6c">
<p>
The goal is a model that can both represent a text for classification
purpose and explain its decision. The model learns first to extract
pieces of an input text as justifications (called rationales) that are
tailored to be short, coherent, and yet sufficient for making
efficient predictions. The model can be decomposed into two steps:
</p>
<ul class="org-ul">
<li>The <i>generator</i> specifies a distribution over text fragments as
candidate rationales. In fact, each word of the input text is
associated with a binary hidden random variable to weight its
importance for the next step.</li>
<li>The <i>encoder</i> takes the output of the generator to make the
prediction.</li>
</ul>

<p>
The rationale extraction can be understood as a type of stochastic
attention although architectures and objectives differ.
</p>
</div>

<div id="outline-container-org78fac83" class="outline-3">
<h3 id="org78fac83">Generator</h3>
<div class="outline-text-3" id="text-org78fac83">
<p>
The goal is to associate to an input sequence of words, a sequence of
hidden random variables, where each hidden variable indicates wether the
associated word should be considered as rationale (\(z=1\)) or not (\(z=0\)).
</p>

<p>
Assume the input text is of sequence of words as input: \(\X =
  x_{1}^{\lsrc}\). The model associates to each input word a binary
variable: \(\Z=z_{1}^{\lsrc}\).  The generator reads the input text
with a BiLSTM. To infer the probability of the sequence \(P(\Z|\X)\),
independent and recurrent predictions are explored.
</p>



<blockquote>
<p>
For NMT people this could be called the encoder. Maybe <i>selector</i> or
simply <i>filter</i> could be used. The choice of terminology in this
paper is for me confusing. However&#x2026;<br />
</p>
</blockquote>
</div>
</div>


<div id="outline-container-org205f37d" class="outline-3">
<h3 id="org205f37d">Encoder</h3>
<div class="outline-text-3" id="text-org205f37d">
<p>
Rationales are defined as the set of \(x_t\) such as \(z_t=1\). Therefore
the input for the encoder is a <i>selection</i> of \(\X\).  Then you can pick
your favorite architecture to deal with this input. In the paper, they
used RNNs and pick the last hidden state to make the final prediction
</p>
</div>
</div>

<div id="outline-container-org282dc88" class="outline-3">
<h3 id="org282dc88">Joint optimization</h3>
<div class="outline-text-3" id="text-org282dc88">
<p>
From an input \(\X\) of length \(\lsrc\), it generates \(\lsrc\)  binary variables \(\Z\). The
generator estimate \(P(\Z|\X)\). 
</p>

<p>
The authors first define a cost function as follows:
</p>
\begin{align}
  cost(\x,\z,\y) &= || \y - f_{\pa_{e}}(\x,\z) ||^2 + \lambda_1 ||\z|| + \lambda_2 \sum_t |z_t - z_{t-1} |  \\
  P(\Z=\z | \X=\x) &= g_{\pa_g}(\x)
\end{align}

<p>
The cost function depends therefore on the value of \(\z\) in three
ways: 
</p>
<ul class="org-ul">
<li>First the term \(||\y - f_{\pa_{e}}(\x,\z) ||^2\) is the
reconstruction error. The target is \(\y\) while the encoder predicts
\(f_{\pa_{e}}(\x,\z)\).</li>
<li>Then, the term \(||\z||\) ensures that the selection (the number of \(z\)
set to one) is as small as possible.</li>
<li>The last term \(\sum_t |z_t - z_{t-1} |\) favors contiguous selection
(phrases).</li>
</ul>

<p>
The loss function to be optimized for each training example is : 
</p>
\begin{align}
\mathcal{L}(\pa_g, \pa_e, \x, \y) &= E_{\z\sim P(\Z|\X)} cost(\x,\y,\z) \\
&=\sum_{\z} P(\Z=\z | \X=\x) cost(\x,\y,\z) \\
&= \sum_{\z} g_{\pa_g}(\x)  cost(\x,\y,\z)
\end{align}

<p>
This expected cost is a workaround to deal with hidden variables.
Minimizing the expected cost is challenging since it involves summing
over all the possible choices of rationales \(\z\). Then the authors
propose to sample \(\z\) from the generator to approximate the expectation.
</p>

<p>
However, the derivatives look bit wired. The cost function is considered
as a constant <i>wrt</i> of the generator parameters. The term related to the
norm of \(\z\) for instance, implies the expected norm of \(\z\). This expectation
depends on the same parameters and could be included in the gradient ?
</p>

<p>
In fact, the assumption made through the paper is: given \(p(\Z|\X)\), \(\z\) is sampled and
becomes then deterministic.
</p>


<p>
To summarize the inference step for training : 
</p>
<ul class="org-ul">
<li>Forward propagation of \(\x\) through the generator gives you \(p(\Z|\X)\).</li>
<li>Sample a bunch of \(\z\) in this distribution.</li>
<li>Given \(\z\), build the input of the encoder and compute the</li>
</ul>
<p>
expected cost.
</p>
<ul class="org-ul">
<li>Update the parameters of the whole model given the expected gradients.</li>
</ul>
</div>
</div>

<div id="outline-container-orgdca14e0" class="outline-3">
<h3 id="orgdca14e0">Inference</h3>
<div class="outline-text-3" id="text-orgdca14e0">
<p>
While for inference: 
</p>
<ul class="org-ul">
<li>Forward propagation of \(\x\) through the generator gives you \(p(\z|\x)\).</li>
<li>Compute \(\z\) and then get the rationales (how ?)</li>
<li>Given \(\z\), build the input of the encoder and compute the answer.</li>
</ul>
</div>
</div>
</div>



<div id="outline-container-org5372019" class="outline-2">
<h2 id="org5372019">Questions / Comments</h2>
<div class="outline-text-2" id="text-org5372019">
</div>
<div id="outline-container-orgeb52df1" class="outline-3">
<h3 id="orgeb52df1">Rationale</h3>
<div class="outline-text-3" id="text-orgeb52df1">
<p>
As written in the paper the notion of what counts as a rationale may
be ambiguous in some contexts and the task of selecting rationales
may therefore be challenging to evaluate. In the paper, they focus on two domains
where ambiguity is minimal (or can be minimized).
</p>
</div>
</div>


<div id="outline-container-org729b6aa" class="outline-3">
<h3 id="org729b6aa">Training and inference</h3>
<div class="outline-text-3" id="text-org729b6aa">
<ul class="org-ul">
<li>For training, maybe I missed it but, there's no mention in the paper of the number of samples  used to approximate the expectation.</li>

<li>The inference step raises a similar question:  but how the second step is implemented ? Just apply a threshold on the probability ? 0.5 ?</li>
</ul>
</div>
</div>


<div id="outline-container-org48235f2" class="outline-3">
<h3 id="org48235f2">Attention based model for classification</h3>
<div class="outline-text-3" id="text-org48235f2">
<p>
The rationale extraction can be understood as attention, even
architectures and objectives differ. The discussion in the paper is
not so convincing for me. It could be interesting to investigate that
point, starting by <a href="https://arxiv.org/abs/1511.05234">this paper on stochastic attention</a>, followed by
<a href="https://arxiv.org/abs/1706.03762">attention is all you need</a>.
</p>
</div>
</div>




<div id="outline-container-orgf9cfa9b" class="outline-3">
<h3 id="orgf9cfa9b">Encoder</h3>
<div class="outline-text-3" id="text-orgf9cfa9b">
<p>
The encoder is a "simple" rnn and the last hidden state is taken as
input for classification. While we expect a short sequence after the
rationale extraction step. This could maybe bias the whole model to
select word at end of the sequence, and then by backprop to favor
rationale extraction at the end.
</p>
</div>
</div>

<div id="outline-container-orgea3338d" class="outline-3">
<h3 id="orgea3338d">Loss function etc &#x2026;</h3>
<div class="outline-text-3" id="text-orgea3338d">
<p>
Note that the first regularization term could be l1 norm instead of l2, to favor sparsity. Maybe good for long documents.
</p>


<p>
More generally, is this formulation the best option ? Since \(\Z\) are hidden variables, could we adapt the Variational Auto Encoder to this task and can we use reparametrization trick ?
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-11-03</p>
<p class="author">Author: A. Allauzen</p>
<p class="date">Created: 2024-06-24 lun. 17:13</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
