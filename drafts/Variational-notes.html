---
title: "[PREVIEW] Variational Bayes : some basics"
date: 2017-07-21
layout: post
categories: 
tags: 
- article 
- bayes 
- variational 
- EM
published: false
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org834ad38">1. Variational Inference: A Review for Statisticians</a>
<ul>
<li><a href="#org6ae6ac4">1.1. Comparing variational inference and MCMC.</a></li>
</ul>
</li>
<li><a href="#orgc4d789b">2. The problem of approximate inference</a>
<ul>
<li><a href="#org988b174">2.1. The evidence lower bound (ELBO)</a></li>
</ul>
</li>
</ul>
</div>
</div>
<span style="background: red;">PREVIEW</span><hr>
<p>
\(\newcommand{\X}{\mathbf{X}}\)
\(\newcommand{\Z}{\mathbf{Z}}\)
\(\newcommand{\pa}{\mathbf{\theta}}\)
\(\newcommand{\paold}{\pa^{old}}\)
\(\newcommand{\panew}{\pa^{new}}\)
</p>

<p>
\(\newcommand{\lb}{\mathcal{L}(q;\pa)}\) 
</p>

<p>
\(\newcommand{\dkl}{D_{kl}}\)
</p>

<div id="outline-container-org834ad38" class="outline-2">
<h2 id="org834ad38"><span class="section-number-2">1</span> Variational Inference: A Review for Statisticians</h2>
<div class="outline-text-2" id="text-1">
<p>
David M. Blei et al. on arxiv 
</p>
</div>

<div id="outline-container-org6ae6ac4" class="outline-3">
<h3 id="org6ae6ac4"><span class="section-number-3">1.1</span> Comparing variational inference and MCMC.</h3>
<div class="outline-text-3" id="text-1-1">
<p>
When should a statistician use MCMC and when should she use variational inference? We will offer some guidance. MCMC methods tend to be more computationally intensive than variational inference but they also provide guarantees of producing (asymptotically) exact samples from the target density (Robert and Casella, 2004). Variational inference does not enjoy such guarantees it can only find a density close to the target but tends to be faster than MCMC. Because it rests on optimization, variational inference easily takes advantage of methods like stochastic opti- mization (Robbins and Monro, 1951; Kushner and Yin, 1997) and distributed optimization (though some MCMC methods can also exploit these innovations (Welling and Teh, 2011; Ahmed et al., 2012)).  
</p>

<p>
Thus, variational inference is suited to large data sets and scenarios where we want to quickly explore many models; MCMC is suited to smaller data sets and scenarios where we happily pay a heavier computational cost for more precise samples. For example, we might use MCMC in a setting where we spent 20 years collecting a small but expensive data set, where we are confident that our model is appropriate, and where we require precise inferences. We might use variational inference when fitting a probabilistic model of text to one billion text documents and where the inferences will be used to serve search results to a large population of users. In this scenario, we can use distributed computation and stochastic optimization to scale and speed up inference, and we can easily explore many different models of the data.
</p>

<p>
Data set size is not the only consideration. Another factor is the geometry of the posterior distribution. For example, the posterior of a mixture model admits multiple modes, each corresponding label permutations of the components. Gibbs sampling, if the model permits, is a powerful approach to sampling from such target distributions; it quickly focuses on one of the modes. For mixture models where Gibbs sampling is not an option, variational inference may perform better than a more general MCMC technique (e.g., Hamiltonian Monte Carlo), even for small datasets (Kucukelbir et al., 2015). Exploring the interplay between model complexity and inference (and between variational inference and MCMC) is an exciting avenue for future research (see Section 5.4).
</p>
</div>
</div>
</div>



<div id="outline-container-orgc4d789b" class="outline-2">
<h2 id="orgc4d789b"><span class="section-number-2">2</span> The problem of approximate inference</h2>
<div class="outline-text-2" id="text-2">
<p>
In this part, there is no mention of hyperparameters or even parameters. Let say that the latent variables represent all unknown quantities of interest.
</p>

<p>
A generative model defines a joint density of latent variables \(\Z\) and observations \(\X\), \[p(\Z, \X) = p(Z)p(\X | \Z)\], with a generative scenario: draw the latent variables from a prior density \(p(\Z)\) and then relates them to the observations through the likelihood $p(\X|\Z). Inference in a Bayesian model amounts to conditioning on data and computing the posterior $p(\Z | \X). In fact, the latent variables \(\Z\) help govern the distribution of the data.
</p>

<p>
To estimate \(p(\Z|\X)\), the Bayes formula tells us: 
</p>

<p>
\[ p(\Z|\X) = \frac{p(\X,\Z)}{p(\X)}$ \] 
</p>

<p>
This is not of a great help since the denominator is the evidence and is also untractable: the summation over all the possible values of \(\Z\).
</p>
</div>
<div id="outline-container-org988b174" class="outline-3">
<h3 id="org988b174"><span class="section-number-3">2.1</span> The evidence lower bound (ELBO)</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Given a family of distribution, we are looking for one its member \(q(\Z)\) that best approximates the conditional distribution \(p(Z|X)\). The criterion is the Kullback-Leibler divergence: 
</p>
<div class="org-center">
<p>
$q<sup>*</sup>(\Z) = argmin<sub>q</sub> \dkl(q(\Z)||p(\Z|\X)) $ 
</p>
</div>
<p>
This objective is not tractable since its relies on $ p(\Z|\X)$ and hence on \(p(\X)\) : 
</p>
\begin{align}
\dkl(q(\Z)||p(\Z|\X)) &= E_{q}[log q(\Z)] - E_{q}[log p(\X,\Z)]\\
&=  E_{q}[log q(\Z)] - E_{q}[log p(\X,Z)] + E_{q}[log p(\X)]\\
&=  E_{q}[log q(\Z)] - E_{q}[log p(\X,Z)] + log p(\X)\\
\end{align}
<p>
This shows the dependence of the objective on the data evidence, but also that \[\dkl(q(\Z)||p(\Z|\X)) \ge E_{q}[log q(\Z)] - E_{q}[log p(\X,\Z)]\]. Minimizing the second term is equivalent to minimize the KL divergence objective (\(log p(\X)\) is independent of \(q(\Z)\) and thus a negative constant).
</p>

<p>
The quantity \(E_{q}[log p(\X,\Z)]- E_{q}[log q(\Z)]\) is called the evidence lower bound (ELBO).  The term ELBO comes from the fact that if you reorganise the equation, you get: 
</p>


<p>
\[ log p(\X) = \dkl(q(\Z)||p(\Z|\X)) + E_{q}[log p(\X,\Z)]- E_{q}[log q(\Z)] \]
</p>
</div>
</div>
</div>
