---
title: "Apprentissage structuré et Deep-Learning / Master AIC"
date: 2018-11-19 21:37:27
layout: page
categories: 
tags: 
- AIC18 
- cours 
- NNet
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org77948ef">1. Evaluation</a></li>
<li><a href="#org7e512e1">2. 21/11/18: Introduction + Bag of words</a>
<ul>
<li><a href="#orgcb18046">2.1. Cours d'intro</a></li>
<li><a href="#orga220007">2.2. TP : prise en main</a></li>
<li><a href="#orge6099f2">2.3. Lecture</a></li>
</ul>
</li>
<li><a href="#orgfa0cce2">3. 28/11/18: Modèle de séquence  - 1</a>
<ul>
<li><a href="#orgf46c976">3.1. Cours : ngram et convolution</a></li>
<li><a href="#org068cb75">3.2. TP : Convolutional Neural Networks for Sentence Classification</a></li>
<li><a href="#orge45e7c0">3.3. Lecture</a></li>
</ul>
</li>
<li><a href="#org9d41bad">4. 05/12/18: Modèle de séquence  - 2</a>
<ul>
<li><a href="#orgbfc833a">4.1. Lectures d'article</a></li>
<li><a href="#orga8376d0">4.2. Cours : Réseaux récurrents et LSTM</a></li>
</ul>
</li>
<li><a href="#org1d1d835">5. 12/12/18: Lectures et TP</a>
<ul>
<li><a href="#org58e1c2f">5.1. Lectures d'article</a></li>
<li><a href="#orgdb22ddf">5.2. TPs</a></li>
</ul>
</li>
<li><a href="#orgae93936">6. 09/09/19:</a>
<ul>
<li><a href="#org4282db2">6.1. Lecture d'article</a></li>
</ul>
</li>
<li><a href="#org2c4ffae">7. 16/01/19 : Neural Parsing</a></li>
<li><a href="#org2a1db15">8. 30/01/19 :</a>
<ul>
<li><a href="#org1f0c2f0">8.1. Lecture d'article</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>


<p>
Les slides et TP sont accessibles via le drive
habituel (<a href="https://drive.google.com/drive/folders/1CSbSdNdxWngB2IHg46dfJxzIEdCsB3n0?usp=sharing">lien direct</a>). Les TP seront en <a href="http://pytorch.org/">pytorch</a>. 
</p>

<div id="outline-container-org77948ef" class="outline-2">
<h2 id="org77948ef"><span class="section-number-2">1</span> Evaluation</h2>
<div class="outline-text-2" id="text-1">
<p>
Le cours sera évalué par une présentation d'article et un projet. 
Vous trouverez les détails <a href="https://docs.google.com/document/d/112kfYZHsSJviIydWrUIGN8oPymuchgk1WJWEW4ouJqA/edit?usp=sharing">ce document partagé</a>. Les dates importantes: 
</p>
<ul class="org-ul">
<li>Choix de l'article par mail avant le 28/11</li>
<li>Choix du projet par mail avant le 12/12</li>
</ul>
</div>
</div>




<div id="outline-container-org7e512e1" class="outline-2">
<h2 id="org7e512e1"><span class="section-number-2">2</span> 21/11/18: Introduction + Bag of words</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-orgcb18046" class="outline-3">
<h3 id="orgcb18046"><span class="section-number-3">2.1</span> Cours d'intro</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Le <b>cours</b> est une intro à l'apprentissage à la modélisation d'objet
structuré. L'exemple principale sera des applications aux données
textuelles.
</p>
</div>
</div>
<div id="outline-container-orga220007" class="outline-3">
<h3 id="orga220007"><span class="section-number-3">2.2</span> TP : prise en main</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Pytorch101 et classification de texte basique. 
</p>
</div>
</div>


<div id="outline-container-orge6099f2" class="outline-3">
<h3 id="orge6099f2"><span class="section-number-3">2.3</span> Lecture</h3>
<div class="outline-text-3" id="text-2-3">
<p>
<a href="https://arxiv.org/abs/1510.00726">A Primer on Neural Network Models for Natural Language Processing</a> is
"short" introduction. <a href="https://u.cs.biu.ac.il/~yogo/">Yoav Goldberg</a> also published <a href="https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037">a book</a> on this
topic. 
</p>
</div>
</div>
</div>


<div id="outline-container-orgfa0cce2" class="outline-2">
<h2 id="orgfa0cce2"><span class="section-number-2">3</span> 28/11/18: Modèle de séquence  - 1</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgf46c976" class="outline-3">
<h3 id="orgf46c976"><span class="section-number-3">3.1</span> Cours : ngram et convolution</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Le <b>cours</b> introduit le problème de la modélisation de
séquence. L'application pour commencer est un modèle de langue, soit
un modèle génératif de séquence de symbole discret. Cette première
partie présente le modèle ngram dans sa version neuronales. 
</p>

<p>
La dernière partie est une étude de cas sur <a href="https://www.aclweb.org/anthology/D14-1181">cet article</a>. 
</p>
</div>
</div>

<div id="outline-container-org068cb75" class="outline-3">
<h3 id="org068cb75"><span class="section-number-3">3.2</span> TP : Convolutional Neural Networks for Sentence Classification</h3>
<div class="outline-text-3" id="text-3-2">
<p>
reproduire le modèle  <a href="https://www.aclweb.org/anthology/D14-1181">cet article</a> sur les données IMDB. 
</p>
</div>
</div>

<div id="outline-container-orge45e7c0" class="outline-3">
<h3 id="orge45e7c0"><span class="section-number-3">3.3</span> Lecture</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Le grand classique: <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>. À lire ! 
</p>
</div>
</div>
</div>


<div id="outline-container-org9d41bad" class="outline-2">
<h2 id="org9d41bad"><span class="section-number-2">4</span> 05/12/18: Modèle de séquence  - 2</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgbfc833a" class="outline-3">
<h3 id="orgbfc833a"><span class="section-number-3">4.1</span> Lectures d'article</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Un peu de classification d'image avec :
</p>
<ul class="org-ul">
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a></li>
<li><a href="https://arxiv.org/abs/1512.03385">ResNet</a></li>
<li><a href="http://proceedings.mlr.press/v37/ioffe15.pdf">BatchNorm</a></li>
</ul>

<p>
Remarque: pour les réseaux avec des architecture différentes
(récurrents, transformers), la BatchNorm est moins utilisée et la
<a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a> semble plus adaptée. 
</p>
</div>
</div>

<div id="outline-container-orga8376d0" class="outline-3">
<h3 id="orga8376d0"><span class="section-number-3">4.2</span> Cours : Réseaux récurrents et LSTM</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Pour les LSTMs, la référence reste la thèse d'<a href="https://www.cs.toronto.edu/~graves/phd.pdf">Alex Graves</a>. 
</p>
</div>
</div>
</div>

<div id="outline-container-org1d1d835" class="outline-2">
<h2 id="org1d1d835"><span class="section-number-2">5</span> 12/12/18: Lectures et TP</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org58e1c2f" class="outline-3">
<h3 id="org58e1c2f"><span class="section-number-3">5.1</span> Lectures d'article</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li><a href="https://arxiv.org/abs/1706.03762">Transformer / Attention is all you need</a></li>
<li><a href="http://www.aclweb.org/anthology/P17-1052">Deep Pyramidal convolution for text classification</a></li>
<li><a href="https://arxiv.org/pdf/1610.02357.pdf">Xception</a>, un peu d'image pour changer.</li>
</ul>

<p>
En complément, il est important d'avoir bien compris le papier sur les
Transformer. Regarder par exemple <a href="https://jalammar.github.io/illustrated-transformer/">cette illustration bien faite</a>. Dans
ce post, tout n'est pas forcément précis mais cela donne une bonne
idée. 
</p>
</div>
</div>

<div id="outline-container-orgdb22ddf" class="outline-3">
<h3 id="orgdb22ddf"><span class="section-number-3">5.2</span> TPs</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Reprises des derniers TPs. La correction du TP1 est disponible. 
</p>
</div>
</div>
</div>


<div id="outline-container-orgae93936" class="outline-2">
<h2 id="orgae93936"><span class="section-number-2">6</span> 09/09/19:</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org4282db2" class="outline-3">
<h3 id="org4282db2"><span class="section-number-3">6.1</span> Lecture d'article</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1805.04655.pdf">Learning to Ask Good Questions: Ranking Clarification Questions
using Neural Expected Value of Perfect Information</a> (Best paper
ACL 2018)</li>
<li><a href="https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf"> Rationalizing Neural Predictions</a></li>
<li><a href="http://aclweb.org/anthology/P18-1082">Hierarchical Neural Story Generation</a></li>
</ul>
</div>
</div>
</div>




<div id="outline-container-org2c4ffae" class="outline-2">
<h2 id="org2c4ffae"><span class="section-number-2">7</span> 16/01/19 : Neural Parsing</h2>
<div class="outline-text-2" id="text-7">
<p>
Benoit Crabbé
</p>
</div>
</div>

<div id="outline-container-org2a1db15" class="outline-2">
<h2 id="org2a1db15"><span class="section-number-2">8</span> 30/01/19 :</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org1f0c2f0" class="outline-3">
<h3 id="org1f0c2f0"><span class="section-number-3">8.1</span> Lecture d'article</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Les deux papiers sur WaveNet
</p>
</div>
</div>
</div>
