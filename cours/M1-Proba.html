---
title: "M1 : Probabilité Statistique et Théorie de l'Information"
date: 2018-12-20
layout: post
categories: 
tags: 
- cours 
- M1 
- ProbaStat
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org414f34b">Séance 1</a></li>
<li><a href="#org3e626f4">Séance 2</a></li>
<li><a href="#orgab7d3bd">Séance 3</a></li>
<li><a href="#org46374d7">Séance 4</a></li>
<li><a href="#orgf4d4926">Séance 5</a></li>
<li><a href="#orgc0678c3">Séance 6</a></li>
<li><a href="#org5e8bb6b">Séance 7</a></li>
<li><a href="#org060ccd0">Séance 8</a></li>
<li><a href="#org3266900">Séance 9</a></li>
<li><a href="#orgd47cf5b">Séance 10: Soutenance</a></li>
<li><a href="#org29a3f89">Projet et soutenance</a>
<ul>
<li><a href="#org816ede9">Consignes</a></li>
<li><a href="#orge2e535c">Exercices</a>
<ul>
<li><a href="#orgc843dfd">Hypothèse Gaussienne sur MNIST (7 pts)</a></li>
<li><a href="#org8336092">Bayésien Naif de Bernoulli (8 pts)</a></li>
<li><a href="#orgbf57880">Clustering: Mélange de Bernoulli (8 pts)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<hr>

<p>
Site du cours de M1 <i>Probabilité Statistique et Théorie de
l'Information</i> en construction. 
</p>

<p>
Les ressources liées au cours sont accessibles sur <a href="https://drive.google.com/open?id=1gQYVlPZLJqwxwUaCNHih8_Ej-FGh7Rwg">ce drive</a>. 
</p>


<div id="outline-container-org414f34b" class="outline-2">
<h2 id="org414f34b">Séance 1</h2>
<div class="outline-text-2" id="text-org414f34b">
<p>
Le 14/01/19: 
</p>
<ul class="org-ul">
<li>Cours: introduction et notions de bases</li>
<li>TP : Intro à python</li>
</ul>

<p>
Les TP utiliseront pour la plupart des <i>notebooks</i>. Un notebook a
l'extension <b><b>.ipynb</b></b> est il se manipule via l'outil
<i>jupyter-notebook</i>: 
</p>
<ul class="org-ul">
<li>lancer un server <i>jupyter-notebook</i></li>
<li>utiliser le navigateur internet pour se connecter à ce server</li>
</ul>
</div>
</div>


<div id="outline-container-org3e626f4" class="outline-2">
<h2 id="org3e626f4">Séance 2</h2>
<div class="outline-text-2" id="text-org3e626f4">
<p>
Le 21/01/19: 
</p>
<ul class="org-ul">
<li>Cours : introduction au probabilité</li>
<li>TD/TP : Exercices sur les probabilités et intro à <i>numpy</i></li>
</ul>
</div>
</div>

<div id="outline-container-orgab7d3bd" class="outline-2">
<h2 id="orgab7d3bd">Séance 3</h2>
<div class="outline-text-2" id="text-orgab7d3bd">
<p>
Le 23/01/19
</p>
<ul class="org-ul">
<li>Cours : fin des probabilités (variable continue)</li>
<li>TD/TP : Exercices sur les probabilités et fin du TP sur MNIST et les
corrélations</li>
</ul>
</div>
</div>


<div id="outline-container-org46374d7" class="outline-2">
<h2 id="org46374d7">Séance 4</h2>
<div class="outline-text-2" id="text-org46374d7">
<p>
Le 04/02/19, TP noté : Recherche Bayesienne. Le notebook associé est
sur le drive. 
</p>

<p>
<b><b>Travail à rendre</b></b>: le notebook avec vos contributions à m'envoyer
par mail avant le cours suivant du 11/02. 
</p>
</div>
</div>

<div id="outline-container-orgf4d4926" class="outline-2">
<h2 id="orgf4d4926">Séance 5</h2>
<div class="outline-text-2" id="text-orgf4d4926">
<p>
Le 11/02/19
</p>
<ul class="org-ul">
<li>Cours: Classication Bayesienne, Bayesien Naif</li>
<li>TP: Bayésien Naif Gaussien (+ Bernoulli) sur les images MNIST</li>
</ul>
</div>
</div>


<div id="outline-container-orgc0678c3" class="outline-2">
<h2 id="orgc0678c3">Séance 6</h2>
<div class="outline-text-2" id="text-orgc0678c3">
<p>
Le 18/02/19
</p>
<ul class="org-ul">
<li>Cours: modéle de mélange de gaussienne, alogorithme EM</li>
<li>TP : Implémentation d'EM pour des GMM sur l'exemple du cours</li>
</ul>
</div>
</div>

<div id="outline-container-org5e8bb6b" class="outline-2">
<h2 id="org5e8bb6b">Séance 7</h2>
<div class="outline-text-2" id="text-org5e8bb6b">
<p>
Le 11/03/19
</p>
<ul class="org-ul">
<li>Cours: Théorie de l'information, incertitude, et entropie</li>
<li>TP: Le codage d'Huffman</li>
</ul>
</div>
</div>

<div id="outline-container-org060ccd0" class="outline-2">
<h2 id="org060ccd0">Séance 8</h2>
<div class="outline-text-2" id="text-org060ccd0">
<p>
Le 18/03/19 
</p>
<ul class="org-ul">
<li>Cours: Théorie de l'information, entropie conditionnelle,
information mutuelle</li>
<li>TP: Arbre de décision (avec sklearn)</li>
</ul>
</div>
</div>

<div id="outline-container-org3266900" class="outline-2">
<h2 id="org3266900">Séance 9</h2>
<div class="outline-text-2" id="text-org3266900">
<p>
Le 01/04/19 
</p>
<ul class="org-ul">
<li>TP : Projet, préparation, soutien, questions &#x2026;</li>
</ul>
</div>
</div>


<div id="outline-container-orgd47cf5b" class="outline-2">
<h2 id="orgd47cf5b">Séance 10: Soutenance</h2>
<div class="outline-text-2" id="text-orgd47cf5b">
<p>
Le 15/04/19 
</p>
</div>
</div>

<div id="outline-container-org29a3f89" class="outline-2">
<h2 id="org29a3f89">Projet et soutenance</h2>
<div class="outline-text-2" id="text-org29a3f89">
</div>
<div id="outline-container-org816ede9" class="outline-3">
<h3 id="org816ede9">Consignes</h3>
<div class="outline-text-3" id="text-org816ede9">
<p>
Ce travail est à faire en binôme ou trinôme. Ce projet fera objet
d'une soutenance qui aura lieu lors de la dernière séance de cours, le
15/04. Vous devez, avant le 14/04 rendre un notebook avec vos
contributions (ou si vous préférez un rapport au format pdf et un
fichier source pour le code). La rédaction du notebook, ou du rapport,
compte pour une part importante. Elle doit mettre en évidence votre
reflexion et votre démarche expérimentale. De plus le code doit être
aussi soigné. 
</p>

<p>
Lors de la soutenance, vous devrez présenter rapidement ce que vous
avez fait et les résultats importants, en 10 minutes maximum. Il
faudra donc sélectionner les points présentés. Par exemple, les réponses aux questions "de
cours", doivent être mise dans le rapport ou le notebook, mais pas
nécessairement dans la présentation. La soutenance sera complétée par
10 minutes de questions portant sur les exercices et donc le cours
(partie théorique et pratique). 
</p>

<p>
Vous pouvez utiliser votre propre implémentation et/ou utiliser des
librairies existantes. Dans ce cas, il suffit de le mentionner.
</p>

<p>
Le total des points excède 20 points. Les points qui dépasseraient
le maximum seront reportés sur les autres notes concernant les travaux
rendus.
</p>
</div>
</div>

<div id="outline-container-orge2e535c" class="outline-3">
<h3 id="orge2e535c">Exercices</h3>
<div class="outline-text-3" id="text-orge2e535c">
</div>
<div id="outline-container-orgc843dfd" class="outline-4">
<h4 id="orgc843dfd">Hypothèse Gaussienne sur MNIST (7 pts)</h4>
<div class="outline-text-4" id="text-orgc843dfd">
<p>
Lors d'un TP précédent, nous avons implémenté un classifieur Bayésien
Naif sur les données MNIST. Deux hypothèses ont alors été  faites: 
</p>
<ul class="org-ul">
<li>Conditionnellement à la classe, chaque pixel est indépendant des autres.</li>
<li>Conditionnellement à la classe, chaque pixel a une densité de
probabilité gaussienne.</li>
</ul>

<p>
Considérons la seconde hypothèse ci-dessus et regardons si cette
hypothèse est juste. L'idée est de chercher les pixels qui ne seraient
pas "Gaussiens". 
</p>
<ul class="org-ul">
<li>Pour la classe 4, sélectionner quelques pixels à grande variance
et tracer leur histogramme.  Commenter les résultats.</li>
<li>Faire de même pour les classes 0,1,2 et 3. Commenter les résultats.</li>
</ul>

<p>
Pour améliorer le classifieur, nous allons utiliser une méthode d'estimation
de densité de probabilité plus fine pour chaque pixel: au lieu de la
considérer comme gaussienne, nous allons donc considérer un mélange de
\(K\) gaussiennes. Pour cela, une suggestion est de commencer par une
par une classe (par exemple la classe 5): 
</p>
<ul class="org-ul">
<li>Pour une classe, on sélectionne \(N\) pixels à forte variance (par
exemple \(N=10\)).</li>
<li>Ecrire la foncion qui implémente l'algorithme E.M (en partant du TP5
par exemple), afin d'estimer le mélange de \(K\) gaussiennes sur cette
sélection de pixels.</li>
<li>Puis faire cela pour toutes les classes, et intégrer les mélanges de
gaussiennes au classifieur Bayésien.</li>
<li>Evaluer le nouveau classifieur, en faisant varier d'abord $K=2,4,6,&#x2026;$ puis $N=10,20,50,100,&#x2026;$.</li>
</ul>
</div>
</div>

<div id="outline-container-org8336092" class="outline-4">
<h4 id="org8336092">Bayésien Naif de Bernoulli (8 pts)</h4>
<div class="outline-text-4" id="text-org8336092">
<p>
Dans le TP sur le Bayésien Naif, il y a un exercice sur
l'implémentation et l'expérimentation d'un classifieur Bayésien Naif,
version Bernoulli, pour les images binarisées. Vous deviez le faire,
c'est l'occasion. 
</p>

<ul class="org-ul">
<li>Un problème est que certains pixels sont constants, en général, ou
pour une classe donnée. Propoer un implémenter une
solution.</li>
<li>Comparer les performances de votre classifieur avec le Bayésien Naif
Gaussien du TP4.</li>
<li>Pour cela utiliser les données <i>train</i> pour l'apprentissage et le
<i>test</i> pour l'évaluation.</li>
</ul>
</div>
</div>



<div id="outline-container-orgbf57880" class="outline-4">
<h4 id="orgbf57880">Clustering: Mélange de Bernoulli (8 pts)</h4>
<div class="outline-text-4" id="text-orgbf57880">
<p>
Repartons sur les données MNIST qu'il est donc possible de
binariser. Dans ce cas chaque pixel peut être considéré comme la
réalisation d'une variable aléatoire binaire. Nous allons faire du
clustering sur ces données (<i>train</i>) en ignorant dans la phase
d'estimation les informations sur les étiquettes. Pour faire du
clustering, nous allons utiliser un modèle de mélange de loi de
Bernoulli. 
</p>

<p>
L'hypothèse que chaque pixel est indépendant des autres est
maintenu. Une image est supposée être engendrée par un mélange de
\(K\) distribution de Bernoulli. 
\[
P(\mathbf{X} = \mathbf{x}) = \sum_{k=1}^K \pi_k P(\mathbf{X} = \mathbf{x} ;\mu_k),
\]
avec \((\pi_k,\mu_k)\) les paramètres du mélange. 
</p>


<p>
Si on s'interesse à une des composantes \(k\) du mélange, la probabilité
d'une image est :
\[
 P(\mathbf{X} = \mathbf{x} ;\mu_k) 
  = \prod_{i=1}^d \mu_{ki}^{x_i} (1-\mu_{ki})^{1-x_i} 
\]
</p>

<p>
Chaque composante du mélange est donc une loi de Bernoulli sur les
images et représente un "cluster". L'objectif est d'implémenter
l'algorithme E.M pour ce modèle de mélange afin de de faire du
clustering sur la base d'image MNIST et d'en regarder les résultat.
</p>

<ul class="org-ul">
<li>Implémenter les deux fonctions (pour l'étape E et l'étape M), avec
\(K\) comme paramètre. Pour cela il est important d'écrire les
équations avant. Il faut donc reprendre le cours sur les mélanges de
gaussiennes et ré-écrire les  équations qui définissent ces 2
étapes.</li>
<li>Lancer l'algorithme de clustering sur les images et visualiser le
résultats pour les valeurs de \(K=5,10,15,20,25\). Pour cela il est
possible de représenter l'image qui représente chaque cluster, en
considérant le vecteur \(\mu_k\) comme une image.</li>
<li>Pour les différentes valeurs de \(K\) , comparer les valeurs de la
vraisemblance.</li>
</ul>
</div>
</div>
</div>
</div>
