---
title: "M1 : Probabilité Statistique et Théorie de l'Information"
date: 2018-01-08
layout: post
categories: 
tags: 
- cours 
- M1 
- ProbaStat
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgbf8b485">Travail à rendre</a>
<ul>
<li><a href="#org2e5522d">Soutenances finale le 09/04</a></li>
<li><a href="#org83f2134">Avant le 12/03/2018 13h29</a></li>
<li><a href="#org4f807f6">Avant le 05/03/2018 13h29</a></li>
</ul>
</li>
<li><a href="#org414f34b">Séance 1</a></li>
<li><a href="#org3e626f4">Séance 2</a></li>
<li><a href="#orgaf67ffa">Séance 3</a></li>
<li><a href="#org6eeed4a">Séance 4</a>
<ul>
<li><a href="#org6c3951a">Correction partielle</a></li>
</ul>
</li>
<li><a href="#orgfd95088">Séance 5</a></li>
<li><a href="#org9b635dd">Séance 6</a></li>
<li><a href="#orga04e5e6">Séance 7</a></li>
<li><a href="#org96b71cb">Les dernières séances</a></li>
<li><a href="#orgcc59ef3">Projet à rendre / soutenances</a>
<ul>
<li><a href="#orgbf30d9d">Exercice 1 (8 pts)</a></li>
<li><a href="#org371f0e5">Exercice 2 (10 pts)</a></li>
<li><a href="#org9c03fae">Excercice 3 (7 pts)</a></li>
<li><a href="#org14ac9ac">Consignes</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>

<p>
Site du cours de M1 <i>Probabilité Statistique et Théorie de
l'Information</i> en construction. 
</p>

<p>
Les ressources liées au cours sont accessibles sur <a href="https://ocsync.limsi.fr/index.php/s/fSdulz6kS9dzK3Z">ce drive</a> 
</p>

<div id="outline-container-orgbf8b485" class="outline-2">
<h2 id="orgbf8b485">Travail à rendre</h2>
<div class="outline-text-2" id="text-orgbf8b485">
</div>
<div id="outline-container-org2e5522d" class="outline-3">
<h3 id="org2e5522d">Soutenances finale le 09/04</h3>
<div class="outline-text-3" id="text-org2e5522d">
<p>
Voir plus bas (projet) pour les détails. 
</p>
</div>
</div>
<div id="outline-container-org83f2134" class="outline-3">
<h3 id="org83f2134">Avant le 12/03/2018 13h29</h3>
<div class="outline-text-3" id="text-org83f2134">
<p>
Rendre le notebook du TP sur le codage d'Huffman, bien commenté et
avec une analyse des résultats. 
</p>

<p>
<b><b>N.B:</b></b> Pensez à utiliser un titre de mail facile à filtrer du type
<i>[M1] TP Huffman</i>. 
</p>
</div>
</div>


<div id="outline-container-org4f807f6" class="outline-3">
<h3 id="org4f807f6">Avant le 05/03/2018 13h29</h3>
<div class="outline-text-3" id="text-org4f807f6">
<p>
Rendre un ou 2 notebooks (choisir un ou des noms de fichiers contenant
votre identité). Le travail à faire: 
</p>
<ul class="org-ul">
<li>Le dernier exercice du TP3 sur le Bayesien  Naïf (intitulé Bayésien
naïf discret) avec des consignes détaillées plus bas</li>
<li>Le TP4 sur la recherche Bayésienne</li>
</ul>

<p>
Concernant la fin du TP3, il est important de : 
</p>
<ul class="org-ul">
<li>estimer les paramètres sur les données d'apprentissage (<i>train</i>) et
d'évaluer le résultat (taux d'erreur) sur les données de test
(<i>test</i>).</li>
<li>introduire la notion de lissage (voir le cours) et faire varier la
valeur du coefficient de lissage (le alpha dans le cours) et
regarder l'impact sur les résultats en test.</li>
</ul>
</div>
</div>
</div>






<div id="outline-container-org414f34b" class="outline-2">
<h2 id="org414f34b">Séance 1</h2>
<div class="outline-text-2" id="text-org414f34b">
<p>
Le 8/01/18: 
</p>
<ul class="org-ul">
<li>Cours: introduction et notions de bases</li>
<li>TP : Intro à python</li>
</ul>
</div>
</div>

<div id="outline-container-org3e626f4" class="outline-2">
<h2 id="org3e626f4">Séance 2</h2>
<div class="outline-text-2" id="text-org3e626f4">
<p>
Le 15/01/18: 
</p>
<ul class="org-ul">
<li>Cours : introduction au probabilité</li>
<li>TD/TP : Exercice + intro à numpy</li>
</ul>
</div>
</div>
<div id="outline-container-orgaf67ffa" class="outline-2">
<h2 id="orgaf67ffa">Séance 3</h2>
<div class="outline-text-2" id="text-orgaf67ffa">
<p>
Le 22/01/18:
</p>
<ul class="org-ul">
<li>Cours : suite et fin des cours précédents</li>
<li>TP : Fin du TP sur les probabilités + exercices sur les corrélations</li>
</ul>
</div>
</div>

<div id="outline-container-org6eeed4a" class="outline-2">
<h2 id="org6eeed4a">Séance 4</h2>
<div class="outline-text-2" id="text-org6eeed4a">
<p>
Le 29/01/10
</p>
<ul class="org-ul">
<li>Cours et TP : Bayésien Naif Gaussien sur les images</li>
</ul>
</div>
<div id="outline-container-org6c3951a" class="outline-3">
<h3 id="org6c3951a">Correction partielle</h3>
<div class="outline-text-3" id="text-org6c3951a">
<p>
Estimation des paramètres et affichage 
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #ffffff; background-color: #000000;">means</span> = {}
<span style="color: #ffffff; background-color: #000000;">variances</span> = {}
<span style="color: #ffffff; background-color: #000000;">priors</span> = {}
<span style="color: #00ffff;">for</span> lbl <span style="color: #00ffff;">in</span> <span style="color: #b0c4de;">range</span>(NLABELS):
    <span style="color: #ffffff; background-color: #000000;">subtrain</span> = images[<span style="color: #ffffff; background-color: #000000;">labels</span>==lbl] <span style="color: #ff7f24;"># get the subpart of train for the class</span>
    <span style="color: #ffffff; background-color: #000000;">mean</span> = subtrain.mean(<span style="color: #ffffff; background-color: #000000;">axis</span>=<span style="color: #ffffff; background-color: #000000;">0</span>)   <span style="color: #ff7f24;"># compute the mean </span>
    <span style="color: #00ffff;">print</span>(subtrain.shape)          <span style="color: #ff7f24;">#        </span>
    <span style="color: #ffffff; background-color: #000000;">means</span>[lbl]=mean                <span style="color: #ff7f24;"># store the mean vector in a dict </span>
    <span style="color: #ffffff; background-color: #000000;">var</span> = subtrain.var(<span style="color: #ffffff; background-color: #000000;">axis</span>=<span style="color: #ffffff; background-color: #000000;">0</span>)     <span style="color: #ff7f24;"># compute the variance</span>
    <span style="color: #ffffff; background-color: #000000;">variances</span>[lbl]=var             <span style="color: #ff7f24;">#</span>
    <span style="color: #ffffff; background-color: #000000;">priors</span>[lbl]=labels[<span style="color: #ffffff; background-color: #000000;">labels</span>==lbl].shape[<span style="color: #ffffff; background-color: #000000;">0</span>]
    <span style="color: #ff7f24;">###############################################</span>
    <span style="color: #ff7f24;"># plot everything</span>
    <span style="color: #ffffff; background-color: #000000;">fig</span> = plt.figure()
    <span style="color: #ffffff; background-color: #000000;">a</span>=fig.add_subplot(<span style="color: #ffffff; background-color: #000000;">1</span>,<span style="color: #ffffff; background-color: #000000;">2</span>,<span style="color: #ffffff; background-color: #000000;">1</span>)
    plt.imshow(mean.reshape(<span style="color: #ffffff; background-color: #000000;">28</span>,<span style="color: #ffffff; background-color: #000000;">28</span>) , matplotlib.pyplot.cm.spectral)  
    a.set_title(<span style="color: #ffa07a;">'mean image of the class '</span>+<span style="color: #b0c4de;">str</span>(lbl))
    <span style="color: #ffffff; background-color: #000000;">a</span>=fig.add_subplot(<span style="color: #ffffff; background-color: #000000;">1</span>,<span style="color: #ffffff; background-color: #000000;">2</span>,<span style="color: #ffffff; background-color: #000000;">2</span>)
    plt.imshow(var.reshape(<span style="color: #ffffff; background-color: #000000;">28</span>,<span style="color: #ffffff; background-color: #000000;">28</span>) , matplotlib.pyplot.cm.spectral)
    a.set_title(<span style="color: #ffa07a;">'variance of the class '</span>+<span style="color: #b0c4de;">str</span>(lbl))
    matplotlib.pyplot.show()
    <span style="color: #ff7f24;">################################################</span>
<span style="color: #ffffff; background-color: #000000;">priors</span> = priors/priors.sum() <span style="color: #ff7f24;"># norm prior distribution</span>
</pre>
</div>
<p>
Puis pour l'inférence, le calcul des probabilités <i>a posteriori</i>: 
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #00ffff;">def</span> <span style="color: #87cefa;">computePosteriors</span>(image):
    <span style="color: #ffffff; background-color: #000000;">posteriors</span> = np.zeros([NLABELS,<span style="color: #ffffff; background-color: #000000;">1</span>])
    <span style="color: #00ffff;">for</span> lbl <span style="color: #00ffff;">in</span> <span style="color: #b0c4de;">range</span>(NLABELS):
        <span style="color: #ffffff; background-color: #000000;">mean</span> = means[lbl]
        <span style="color: #ffffff; background-color: #000000;">sigma2</span> = variances[lbl]
        <span style="color: #ffffff; background-color: #000000;">non_null</span> = sigma2!=<span style="color: #ffffff; background-color: #000000;">0</span>
        <span style="color: #ffffff; background-color: #000000;">scale</span> = <span style="color: #ffffff; background-color: #000000;">0</span>.<span style="color: #ffffff; background-color: #000000;">5</span>*np.log(<span style="color: #ffffff; background-color: #000000;">2</span>*sigma2[non_null]*math.pi)
        <span style="color: #ffffff; background-color: #000000;">expterm</span> = -<span style="color: #ffffff; background-color: #000000;">0</span>.<span style="color: #ffffff; background-color: #000000;">5</span>*np.divide(np.square(image[non_null]-mean[non_null])
                     ,sigma2[non_null])
        <span style="color: #ffffff; background-color: #000000;">llh</span> = (expterm-scale).sum()
        <span style="color: #ffffff; background-color: #000000;">post</span> = llh + np.log(priors[lbl]) 
        <span style="color: #ffffff; background-color: #000000;">posteriors</span>[lbl]=post
    <span style="color: #00ffff;">return</span> posteriors
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgfd95088" class="outline-2">
<h2 id="orgfd95088">Séance 5</h2>
<div class="outline-text-2" id="text-orgfd95088">
<p>
Le 05/02/18:
</p>
<ul class="org-ul">
<li>Cours : fin du Bayésien Naif</li>
<li>un TP long sur l'inférence Bayesienne (recherche)</li>
</ul>
</div>
</div>

<div id="outline-container-org9b635dd" class="outline-2">
<h2 id="org9b635dd">Séance 6</h2>
<div class="outline-text-2" id="text-org9b635dd">
<p>
Le 05/03/18
</p>
<ul class="org-ul">
<li>Cours : Information Theory / partie 1</li>
<li>TP : Huffman</li>
</ul>
</div>
</div>


<div id="outline-container-orga04e5e6" class="outline-2">
<h2 id="orga04e5e6">Séance 7</h2>
<div class="outline-text-2" id="text-orga04e5e6">
<p>
Le 12/03/18
</p>
<ul class="org-ul">
<li>Cours : Information Theory / partie 2</li>
<li>TP : Application de l'information mutuelle</li>
</ul>
</div>
</div>

<div id="outline-container-org96b71cb" class="outline-2">
<h2 id="org96b71cb">Les dernières séances</h2>
<div class="outline-text-2" id="text-org96b71cb">
<ul class="org-ul">
<li>8: 19/03</li>
<li>9: 26/03 Rien</li>
<li>10: 09/04 Soutenances</li>
</ul>
</div>
</div>


<div id="outline-container-orgcc59ef3" class="outline-2">
<h2 id="orgcc59ef3">Projet à rendre / soutenances</h2>
<div class="outline-text-2" id="text-orgcc59ef3">
</div>
<div id="outline-container-orgbf30d9d" class="outline-3">
<h3 id="orgbf30d9d">Exercice 1 (8 pts)</h3>
<div class="outline-text-3" id="text-orgbf30d9d">
<p>
Partons de l'exercice 3 (à faire de préférence avant)
</p>

<ul class="org-ul">
<li>Expliciter et expliquer l'hypothèse faite lors de la construction du</li>
</ul>
<p>
classifieur. En particulier, chaque pixel est considéré comme une
variable aléatoire et donc une image comme la réalisation d'un vecteur
aléatoire.  Quelle hypothèse est faite sur la
dépendance entre les pixels ?  Quel est l'impact sur la matrice de covariance ? 
</p>
<ul class="org-ul">
<li>Proposer une mesure permettant d'évaluer cette hypothèse.</li>
<li>Évaluons cette hypothèse en considérant comme voisinage  pour chaque pixel, son
voisin de gauche et de droite.</li>
<li>Si nous faisions l'hypothèse qu'un pixel ne dépend que de ses
voisins immédiats de gauche et de droite, écrire les paramètres à
estimer et comment les estimer.  On pourra se contenter de
dépendances simples: si une image est un vecteur, un pixel (une
composante du vecteur) dépend du pixel avant et après dans le
vecteur; pour les pixels sur les bords du vecteurs peuvent être
ignoré si besoin.</li>

<li>BONUS: Coder et expérimenter ce modèle</li>
</ul>
</div>
</div>



<div id="outline-container-org371f0e5" class="outline-3">
<h3 id="org371f0e5">Exercice 2 (10 pts)</h3>
<div class="outline-text-3" id="text-org371f0e5">
<p>
Repartons du TP sur Bayésien Naif Gaussien appliqué aux images MNIST. 
</p>

<p>
Le classifieur vu en TP considère la réalisation un pixel
conditionnellement à une classe comme une variable aléatoire
gaussienne.
</p>
<ul class="org-ul">
<li>Prenez les images de la classe 4, prendre 5 pixels avec une forte
variance et tracer pour chacun la répartition des valeurs avec un
histogramme.</li>
<li>Faites la même chose pour la classe 5.</li>
<li>Commenter les résultats obtenus</li>
</ul>

<p>
Supposons maintenant que la valeur d'un pixel conditionnellement à sa
classe est en fait une variable aléatoire dont la distribution est
donné par le mélange de 2 gaussiennes: 
\[ X_i|Y=y \sim \alpha_{i,y,1}\mathcal{N}(\mu_{i,y,1},
\sigma_{i,y,1}) + \alpha_{i,y,2}\mathcal{N}(\mu_{i,y,2}, \sigma_{i,y,2})
\]
\[
p(x_i|y) = \alpha_{i,y,1} p_1(x|y)  + \alpha_{i,y,2} p_2(x|y),
\]
avec \(p_1(x|y)   = \mathcal{N}(\mu_{i,y,1}, \sigma_{i,y,1})\). La distribution est en fait l'interpolation linéaire de 2 gaussiennes,
et : 
\[  \alpha_{i,y,1} +  \alpha_{i,y,2}  =1 \]
La question qui se pose est comment calculer les paramètres de ce
nouveau modèle pour une classe \(y\) donnée, à savoir : 
\[(\alpha_{i,y,1},\alpha_{i,y,2},  \mu_{i,y,1} ,  \mu_{i,y,2},
\sigma_{i,y,1} ,\sigma_{i,y,2})\]
et nous allons voir comment au travers des questions suivantes. 
</p>

<p>
Prenons par exemple les images de la classe 5 et un pixel du centre de
l'image (par exemple un des pixels que vous avez sélectionné dans une
des questions précédentes).  Dans le cas à une seule gaussienne,
chaque réalisation contribue à l'estimation des statistiques
nécessaires (\(\mu,\sigma\)). Avec 2 gaussiennes, on ne sais pas à
gaussienne, une observation va contribuer. Mais on peut calculer cette
contribution, si on connait et fixe les paramètres. 
</p>

<ul class="org-ul">
<li>Proposer une fonction permettant de calculer la contribution d'une
réalisation à chacune des gaussienne. Sachant que la somme des deux
contributions est 1 et qu'elles dépendent de \(\alpha_{i,y,1},
  \alpha_{i,y,1}, p_1, p_2\).</li>
<li>Une fois connue cette contribution, il est possible de mettre à jour
les paramètres, puisque l'on sait pour chaque réalisation, de
combien elle participe à chaque gaussienne. Une fois les
contribution de chaque réalisation calculée sur les données
d'apprentissage, écrire la formule d'estimation des paramètres pour</li>
</ul>
<p>
\[(\alpha_{i,y,1},\alpha_{i,y,2},  \mu_{i,y,1} ,  \mu_{i,y,2},
\sigma_{i,y,1} ,\sigma_{i,y,2}).\]
</p>

<p>
Ainsi, si les paramètres sont connus, il est possible de calculer les
contributions de chaque exemple, puis de recalculer les
paramètres. Ainsi l'algorithme d'apprentissage est itératif: 
</p>
<ul class="org-ul">
<li>0 : initialisation des contributions (tirage au hasard entre les
deux gaussiennes)</li>
<li>1 : Calcul des paramètres.</li>
<li>2 : Calcul des contributions et retour à l'étape 1</li>
</ul>

<p>
Cet algorithme s'appelle E.M, et vous allez l'implémenter avec 2
fonctions à coder et à tester. 
</p>
<ul class="org-ul">
<li>Partir des données MNIST et les images de la classe 5, estimer les
paramètres du mélange, visualiser la distribution obtenues pour les
pixels sélectionnés et comparer cette fonction à l'histogramme des
valeurs.</li>
<li>Recommencer avec plusieurs initialisations aléatoires pour comparer
les résultats. Sont-ils stables ?</li>
<li>Faire de même avec la classe 4</li>
<li>BONUS : coder un classifier qui utilise un mélange de 2 gaussiennes
pour modéliser \(X_i|Y\), évaluer ses performances et comparer avec le
classifieur de base.</li>
</ul>
</div>
</div>


<div id="outline-container-org9c03fae" class="outline-3">
<h3 id="org9c03fae">Excercice 3 (7 pts)</h3>
<div class="outline-text-3" id="text-org9c03fae">
<p>
Dans le TP sur le Bayésien Naif, il y a un exercice sur
m'implémentation et l'expérimentation d'un classifieur binomial que
vous deviez faire et rendre. C'est l'occasion. 
</p>
</div>
</div>



<div id="outline-container-org14ac9ac" class="outline-3">
<h3 id="org14ac9ac">Consignes</h3>
<div class="outline-text-3" id="text-org14ac9ac">
<p>
Ce travail est à faire en binôme ou trinôme. Ce projet fera objet
d'une soutenance qui aura lieu lors de la dernière séance de cours, le
09/04. Vous devez, avant le 09/04 rendre un notebook avec vos
contributions (ou si vous préférez un rapport au format pdf et un
fichier source). 
</p>

<p>
Lors de la soutenance, vous devrez présenter rapidement ce que vous
avez fait et les résultats importants, en 10 minutes maximum. Il
faudra sélectionner. Par exemple, les réponses aux questions "de
cours", doivent être mise dans le rapport ou le notebook, mais pas
nécessairement dans la présentation. La soutenance sera complétée par
10 minutes de questions portant sur les exercices et donc le cours
(partie théorique et pratique). 
</p>

<p>
Vous pouvez utiliser votre propre implémentation et/ou bien spur
utiliser des librairies existantes. Dans ce cas, il est important de
le mentionner. 
</p>

<p>
Le total des points excède 20 points. Les points qui dépasseraient
le maximum seront reportés sur les autres notes concernant les travaux
rendus.
</p>
</div>
</div>
</div>
