---
title: "Cours TC4 (Partie Probabiliste)"
date: 2018-08-30 21:36:30
layout: post
categories: 
tags: 
- AIC18 
- cours
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org920e6ca">1. Cours 1 : Rappel de probabilité</a>
<ul>
<li><a href="#org0b08c06">1.1. ToDo avant le cours: le cours</a></li>
<li><a href="#orga49575b">1.2. ToDo avant le cours: exercices</a></li>
</ul>
</li>
<li><a href="#org525268b">2. Cours 2 : Modèle de Markov Caché ou HMM, introduction</a>
<ul>
<li><a href="#orgd35cfe5">2.1. ToDo pour le 16/10</a></li>
</ul>
</li>
<li><a href="#org7d5a5d8">3. Cours 3 : Inférence / Viterbi</a></li>
</ul>
</div>
</div>
<hr>

<p>
Cette page contient les informations pour la partie probabiliste de ce cours (Rappel de probabilité et modèle de Markov). L'autre partie est assurée par J-C Janodet. 
</p>

<div id="outline-container-org920e6ca" class="outline-2">
<h2 id="org920e6ca"><span class="section-number-2">1</span> Cours 1 : Rappel de probabilité</h2>
<div class="outline-text-2" id="text-1">
<p>
Ce cours servira autant au module TC1 (Apprentissage).
</p>
</div>
<div id="outline-container-org0b08c06" class="outline-3">
<h3 id="org0b08c06"><span class="section-number-3">1.1</span> ToDo avant le cours: le cours</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<b>Avant  le premier cours vous devez lire attentivement</b> les 2 cours suivants (<a href="https://drive.google.com/open?id=0B6bFVfow2ez_MjV3cEszTU05XzQ">sur le drive</a>): 
</p>
<ul class="org-ul">
<li>Probabilité: un rappel sur les probabilités telles que l'on va les manipuler en AIC. Selon vos aquis dans le domaine, un cours basique peut vous être utile. Dans ce cas lisez <a href="http://apiacoa.org/publications/teaching/statistics/cours.pdf">le cours de mon collègue F. Rossi</a> qui est très complet.</li>
<li>Classification Bayésienne</li>
</ul>


<p>
Même si vous êtes nombreux à déjà connaitre ces notions (le tout ou
certaines parties). Votre 
objectif est <b>de tout comprendre</b>, et si il y a des points obscures de <b>venir au cours avec des questions</b>. 
</p>
</div>
</div>

<div id="outline-container-orga49575b" class="outline-3">
<h3 id="orga49575b"><span class="section-number-3">1.2</span> ToDo avant le cours: exercices</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Les exercices "papier" sont disponibles sur <a href="https://allauzen.github.io/cours/Exercices_Proba/">cette page</a>. 
</p>
</div>
</div>
</div>





<div id="outline-container-org525268b" class="outline-2">
<h2 id="org525268b"><span class="section-number-2">2</span> Cours 2 : Modèle de Markov Caché ou HMM, introduction</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Introduction au HMM.</li>
</ul>
</div>

<div id="outline-container-orgd35cfe5" class="outline-3">
<h3 id="orgd35cfe5"><span class="section-number-3">2.1</span> ToDo pour le 16/10</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Faire le TP1 qui familiarise avec avec les données que l'on va
utiliser</li>
<li>Préparer Viterbi</li>
</ul>


<p>
Pour l'algorithme de Viterbi, la question qui se pose est comment calculer la meilleure séquence d'étiquettes pour une phrase donnée connaissant les paramètres du HMM. Par meilleure, on entend la séquence d'étiquettes (ou d'états) la plus probable connaissant la séquence d'obervation.
</p>

<p>
Proposer et implémenter un algorithme répondant à cette question. Pour
vous aider à démarrer, cet algorithme s'appelle Viterbi et regardez
cette vidéo <a href="https://www.youtube.com/watch?v=RwwfUICZLsA">https://www.youtube.com/watch?v=RwwfUICZLsA</a>, pour
comprendre comment il opère
</p>
</div>
</div>
</div>

<div id="outline-container-org7d5a5d8" class="outline-2">
<h2 id="org7d5a5d8"><span class="section-number-2">3</span> Cours 3 : Inférence / Viterbi</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>forward / backward</li>
<li>retour sur l'algorithme de Viterbi</li>
</ul>
</div>
</div>
