---
title: "Cours TC4 (Partie Probabiliste)"
date: 2018-10-22
layout: post
categories: 
tags: 
- AIC18 
- cours
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org76bca28">1. Cours 1 : Rappel de probabilité</a>
<ul>
<li><a href="#orge2f1a2b">1.1. ToDo avant le cours: le cours</a></li>
<li><a href="#org1b0122e">1.2. ToDo avant le cours: exercices</a></li>
</ul>
</li>
<li><a href="#orgf34c689">2. Cours 2 : Modèle de Markov Caché ou HMM, introduction</a></li>
<li><a href="#org6994906">3. ToDo pour le 16/10</a></li>
<li><a href="#org4f55d41">4. Cours 3 : Inférence / Viterbi</a></li>
<li><a href="#org92ca539">5. Cours 4 : Forward/Backward</a></li>
<li><a href="#orgb83c8fd">6. Project: Second-Order HMM for typos correction</a>
<ul>
<li><a href="#org40f278d">6.1. TODO:</a></li>
<li><a href="#orgbc2b718">6.2. Submission Checklist</a></li>
<li><a href="#org75905ba">6.3. Grading Scheme:</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>

<p>
Cette page contient les informations pour la partie probabiliste de ce cours (Rappel de probabilité et modèle de Markov). L'autre partie est assurée par J-C Janodet. 
</p>

<div id="outline-container-org76bca28" class="outline-2">
<h2 id="org76bca28"><span class="section-number-2">1</span> Cours 1 : Rappel de probabilité</h2>
<div class="outline-text-2" id="text-1">
<p>
Ce cours servira autant au module TC1 (Apprentissage).
</p>
</div>
<div id="outline-container-orge2f1a2b" class="outline-3">
<h3 id="orge2f1a2b"><span class="section-number-3">1.1</span> ToDo avant le cours: le cours</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<b>Avant  le premier cours vous devez lire attentivement</b> les 2 cours suivants (<a href="https://drive.google.com/open?id=0B6bFVfow2ez_MjV3cEszTU05XzQ">sur le drive</a>): 
</p>
<ul class="org-ul">
<li>Probabilité: un rappel sur les probabilités telles que l'on va les manipuler en AIC. Selon vos aquis dans le domaine, un cours basique peut vous être utile. Dans ce cas lisez <a href="http://apiacoa.org/publications/teaching/statistics/cours.pdf">le cours de mon collègue F. Rossi</a> qui est très complet.</li>
<li>Classification Bayésienne</li>
</ul>


<p>
Même si vous êtes nombreux à déjà connaitre ces notions (le tout ou
certaines parties). Votre 
objectif est <b>de tout comprendre</b>, et si il y a des points obscures de <b>venir au cours avec des questions</b>. 
</p>
</div>
</div>

<div id="outline-container-org1b0122e" class="outline-3">
<h3 id="org1b0122e"><span class="section-number-3">1.2</span> ToDo avant le cours: exercices</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Les exercices "papier" sont disponibles sur <a href="https://allauzen.github.io/cours/Exercices_Proba/">cette page</a>. 
</p>
</div>
</div>
</div>





<div id="outline-container-orgf34c689" class="outline-2">
<h2 id="orgf34c689"><span class="section-number-2">2</span> Cours 2 : Modèle de Markov Caché ou HMM, introduction</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>Cours :  Introduction au HMM.</li>
<li>TP : pas de TP mais &#x2026;</li>
</ul>
</div>
</div>
<div id="outline-container-org6994906" class="outline-2">
<h2 id="org6994906"><span class="section-number-2">3</span> ToDo pour le 16/10</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Faire le TP1 qui familiarise avec avec les données que l'on va
utiliser</li>
<li>Préparer Viterbi</li>
</ul>


<p>
Pour l'algorithme de Viterbi, la question qui se pose est comment calculer la meilleure séquence d'étiquettes pour une phrase donnée connaissant les paramètres du HMM. Par meilleure, on entend la séquence d'étiquettes (ou d'états) la plus probable connaissant la séquence d'obervation.
</p>

<p>
Proposer et implémenter un algorithme répondant à cette question. Pour
vous aider à démarrer, cet algorithme s'appelle Viterbi et regardez
cette vidéo <a href="https://www.youtube.com/watch?v=RwwfUICZLsA">https://www.youtube.com/watch?v=RwwfUICZLsA</a>, pour
comprendre comment il opère
</p>
</div>
</div>

<div id="outline-container-org4f55d41" class="outline-2">
<h2 id="org4f55d41"><span class="section-number-2">4</span> Cours 3 : Inférence / Viterbi</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>Cours sur  Viterbi / forward / backward</li>
<li>TP: modèle de Markov Apprentissage supervisé</li>
</ul>
</div>
</div>

<div id="outline-container-org92ca539" class="outline-2">
<h2 id="org92ca539"><span class="section-number-2">5</span> Cours 4 : Forward/Backward</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>Cours sur  Viterbi / forward / backward, suite et fin.</li>
<li>TP: modèle de Markov Apprentissage supervisé, suite et fin</li>
</ul>
</div>
</div>


<div id="outline-container-orgb83c8fd" class="outline-2">
<h2 id="orgb83c8fd"><span class="section-number-2">6</span> Project: Second-Order HMM for typos correction</h2>
<div class="outline-text-2" id="text-6">
<p>
The goal is to design a model to correct typos in texts without a dictionaries. 
</p>

<p>
In this problem, a state refers to the correct letter that should have
been typed, and an observation refers to the actual letter that is
typed. Given a sequence of outputs/observations (i.e., actually typed
letters), the problem is to reconstruct the hidden state sequence (i.e.,
the intended sequence of letters). Thus, data for this problem looks
like:
</p>

<pre class="example">
[('t', 't'), ('h', 'h'), ('w', 'e'), ('k', 'm')]
 [('f', 'f'), ('o', 'o'), ('r', 'r'), ('m', 'm')] 
</pre>

<p>
The first example is misspelled: the observation is <b>thwk</b> while the
correct word is <b>them</b>. The second example is correctly typed.
</p>

<p>
Data for this problem was generated as follows: starting with a text
document, in this case, the Unabomber's Manifesto, which was chosen not
for political reasons, but for its convenience being available on-line
and of about the right length, all numbers and punctuation were
converted to white space and all letters converted to lower case. The
remaining text is a sequence only over the lower case letters and the
space character, represented in the data files by an underscore
character. Next, typos were artificially added to the data as follows:
with 90% probability, the correct letter is transcribed, but with 10%
probability, a randomly chosen neighbor (on an ordinary physical
keyboard) of the letter is transcribed instead. Space characters are
always transcribed correctly. In a harder variant of the problem, the
rate of errors is increased to 20%.
</p>

<p>
The dataset in an archive, see the shared drive to download it. This
archive contains 4 pickles: train10 and test10 constitute the dataset
with 10% or spelling errors, while train20 and test20 the one with 20%
or errors.
</p>
</div>

<div id="outline-container-org40f278d" class="outline-3">
<h3 id="org40f278d"><span class="section-number-3">6.1</span> TODO:</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>(3pts) Dry run: Train a first-order HMM using the training data. This
is basically what we did in lab sessions for POS-tagging. Compute the
error rate (at the character level) and compare this results with the
dummiest classifier that just do nothing. You can also compute the
number of errors your model can correct and the number of errors your
model creates.</li>
<li>(7pts) Second Order HMM: To improve the performance, we can increase
the order of the HMM. Implement a second Order model for this task
(this means that the probability of the next state depends on the
current state and the previous one as well). A convenient way to
implement a second order HMM, is to think about it as a variable
change.</li>
<li>(5pts) Critics and evolution: This model is somehow limited. For
instance, it only handles substitution errors. Could you describe a
way to extend this model to also handle noisy insertion of characters
?</li>

<li>(5pts) Same question for deletion (or omitted characters) ?</li>

<li>(5pts) Unsupervised training: Propose and discuss some ideas to do
unsupervised training for this task. For this question you should
provide details on : what kind of data, which parameters will be
involved, &#x2026;</li>
</ul>

<p>
For the last three questions, be as precise as you can. An
implementation is not mandatory.
</p>
</div>
</div>

<div id="outline-container-orgbc2b718" class="outline-3">
<h3 id="orgbc2b718"><span class="section-number-3">6.2</span> Submission Checklist</h3>
<div class="outline-text-3" id="text-6-2">
<p>
You can rely on existing libraries such as nltk or sklearn, and you can
use other programming languages. You must send an archive with: 
</p>
<ul class="org-ul">
<li>the source code (without external libraries of course) necessary to</li>
</ul>
<p>
reproduce your results 
</p>
<ul class="org-ul">
<li>a report that presents, explains, analyses and discusses your results.</li>
</ul>

<p>
You can of course use a notebook. 
</p>
</div>
</div>

<div id="outline-container-org75905ba" class="outline-3">
<h3 id="org75905ba"><span class="section-number-3">6.3</span> Grading Scheme:</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Just follow the TODO list. Extra points will be given for an
implementation for the last three questions (5pts each)
</p>
</div>
</div>
</div>
