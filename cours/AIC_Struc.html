---
title: "Deep-Learning / Master AIC"
date: 2017-11-13
layout: page
categories: 
tags: 
- AIC17 
- cours 
- NNet
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org4bb6afd">1. Préparation au cours</a></li>
<li><a href="#orgfcc07e3">2. 21/11/17 : Introduction (A. Allauzen)</a></li>
<li><a href="#org060c0b5">3. 28/11/17 : Modèle de séquence 1 (A. Allauzen)</a></li>
<li><a href="#org37a94d3">4. 05/12/17 : Modèle de séquence 2 (A. Allauzen)</a></li>
<li><a href="#orgdfecdd3">5. 06/12/17 : Modélisation d'espace de sortie à grande dimension (M. Labeau)</a></li>
</ul>
</div>
</div>
<hr>


<p>
L'option démarre avec une semaine de retard. La séance perdue est
<b>rattrappée le 6/12.</b> Les slides et TP sont accessibles via le drive
habituel.  <a href="https://ocsync.limsi.fr/index.php/s/TETPT57yxziIN8R">Voici le lien</a>.  Les TP seront en <a href="http://pytorch.org/">pytorch</a>. 
</p>


<p>
*Attention: * les séances, en général,  démarrent par le TP et se poursuivent par le
cours. 
</p>

<div id="outline-container-org4bb6afd" class="outline-2">
<h2 id="org4bb6afd"><span class="section-number-2">1</span> Préparation au cours</h2>
<div class="outline-text-2" id="text-1">
<p>
La première séance commencera par un TP et suivit d'un cours. D'ici là
vous trouverez sur le répertoire habituel une préparation au TP de la
semaine prochaine. 
</p>

<p>
<a href="https://ocsync.limsi.fr/index.php/s/TETPT57yxziIN8R">https://ocsync.limsi.fr/index.php/s/TETPT57yxziIN8R</a>
</p>

<p>
Cette préparation qui a la forme d'un notebook est à faire <b>avant</b> la
première séance bien sûr. Si vous avez des questions, n'hésitez pas à
m'envoyer un mail ou à utiliser la liste du master. Pour le TP de la semaine prochaine, tout sera considéré comme acquis. 
</p>
</div>
</div>




<div id="outline-container-orgfcc07e3" class="outline-2">
<h2 id="orgfcc07e3"><span class="section-number-2">2</span> 21/11/17 : Introduction (A. Allauzen)</h2>
<div class="outline-text-2" id="text-2">
<p>
Le <b>cours</b> est une intro à l'apprentissage à la modélisation d'objet
structuré. L'exemple principale sera des applications aux données
textuelles. 
</p>
</div>
</div>





<div id="outline-container-org060c0b5" class="outline-2">
<h2 id="org060c0b5"><span class="section-number-2">3</span> 28/11/17 : Modèle de séquence 1 (A. Allauzen)</h2>
<div class="outline-text-2" id="text-3">
<p>
Le <b>cours</b> introduit le problème de la modélisation de
séquence. L'application pour commencer est un modèle de langue, soit
un modèle génératif de séquence de symbole discret. Cette première
partie présente le modèle ngram dans sa version neuronales. 
</p>


<p>
Le <b>TP</b> aborde la classification de texte avec un modèle
rudimentaire. Ce modèle considére un document comme un sac de
caractéristiques. Il ignore donc la structure de l'objet à traiter.
</p>
</div>
</div>


<div id="outline-container-org37a94d3" class="outline-2">
<h2 id="org37a94d3"><span class="section-number-2">4</span> 05/12/17 : Modèle de séquence 2 (A. Allauzen)</h2>
<div class="outline-text-2" id="text-4">
<p>
<b>Le TP</b> porte sur les modèles ngram.  
</p>

<p>
Suite du <b>cours</b> sur les modèles de séquence avec les architectures
récurrentes et les cellules LSTM. 
</p>
</div>
</div>



<div id="outline-container-orgdfecdd3" class="outline-2">
<h2 id="orgdfecdd3"><span class="section-number-2">5</span> 06/12/17 : Modélisation d'espace de sortie à grande dimension (M. Labeau)</h2>
</div>
