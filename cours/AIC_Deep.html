---
title: "Deep-Learning / Master AIC"
date: 2018-10-15
layout: post
categories: 
tags: 
- AIC18 
- cours 
- NNet
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org4735ebc">1. Cours 1 : 03/10, double séance 3h sur les bases du deep-learning</a></li>
<li><a href="#orge916df3">2. TP 1 : 09/10, premier pas</a></li>
<li><a href="#org7ebe8b7">3. Cours 2 : 10/10</a></li>
<li><a href="#orgd0b3062">4. TP2 : le 16/10, 1 couche cachée et back-prop</a>
<ul>
<li><a href="#org0e10feb">4.1. Récapitulation du TP1: sans couche cachée</a></li>
<li><a href="#org6bcbbc9">4.2. TP2: avec une couche cachée</a></li>
<li><a href="#org641b530">4.3. TP2: Roadmap</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>


<p>
Cours fait avec Michèle Sebag. Ce site contient les infos sur ma
(première) partie. 
</p>


<div id="outline-container-org4735ebc" class="outline-2">
<h2 id="org4735ebc"><span class="section-number-2">1</span> Cours 1 : 03/10, double séance 3h sur les bases du deep-learning</h2>
<div class="outline-text-2" id="text-1">
<p>
Pour les premiers cours, nous nous appuierons sur le cours en ligne de Hugo Larochelle.
La page officielle du cours est :
<a href="http://www.dmi.usherb.ca/~larocheh/cours/ift725_A2014/contenu.html">http://www.dmi.usherb.ca/~larocheh/cours/ift725_A2014/contenu.html</a>
</p>

<p>
Les vidéos associées sont accessibles via YouTube:
<a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH</a>
</p>


<p>
Pour la première séance, il est demandé de suivre AVANT les vidéos
numérotées de 1.1 (Artificial neuron) à 2.3 (output layer gradient).
Le travail à faire est donc d'avoir suivit ces cours: regarder et
prendre des notes comme pour un cours "habituel".  La séance du 03/10
partira du principe que ces vidéos ont été visionnées et sera dédiée à
vos questions et à des exercices.
</p>

<p>
Bon visionnage ! 
</p>
</div>
</div>


<div id="outline-container-orge916df3" class="outline-2">
<h2 id="orge916df3"><span class="section-number-2">2</span> TP 1 : 09/10, premier pas</h2>
<div class="outline-text-2" id="text-2">
<p>
Le TP est diponible via un notebook sur le drive habituel 
(<a href="https://drive.google.com/drive/folders/0B6bFVfow2ez_MjV3cEszTU05XzQ">rappel du lien</a>). Pour simplifier les choses, un module python
l'accompagne et il concerne le chargement des données. 
</p>
</div>
</div>


<div id="outline-container-org7ebe8b7" class="outline-2">
<h2 id="org7ebe8b7"><span class="section-number-2">3</span> Cours 2 : 10/10</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Faire les exercices qui sont à la fin des slides</li>
<li>Finir de voir les vidéos jusqu'à la 2.11.</li>
</ul>
</div>
</div>


<div id="outline-container-orgd0b3062" class="outline-2">
<h2 id="orgd0b3062"><span class="section-number-2">4</span> TP2 : le 16/10, 1 couche cachée et back-prop</h2>
<div class="outline-text-2" id="text-4">
<p>
On poursuit le TP précédent en ajoutant une couche cachée et
implémenter la back-prop. 
</p>
</div>


<div id="outline-container-org0e10feb" class="outline-3">
<h3 id="org0e10feb"><span class="section-number-3">4.1</span> Récapitulation du TP1: sans couche cachée</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Les fonctions écrites: 
</p>
<ul class="org-ul">
<li><i>init</i> : initialisation des paramètres (\(\mathbf{W},\mathbf{b}\))</li>
<li><i>forward</i> : à partir d'une entrée \(\mathbf{x}\) calcul
\(\mathbf{a}=\mathbf{W}\mathbf{x}+\mathbf{b}\)</li>
<li><i>softmax</i> : calcul la version stable du softmax</li>
<li><i>grad-out</i> : calcul le gradient de la fonction de perte par rapport
à la pré-activation de sortie, soit</li>
</ul>
<p>
\[\mathbf{\delta} = \frac{\partial \mathcal{L}}{\partial \mathbf{a}}.\]
</p>

<p>
La mise à jour des paramètres se calcul alors à partir de
\(\mathbf{\delta}\). 
</p>
</div>
</div>


<div id="outline-container-org6bcbbc9" class="outline-3">
<h3 id="org6bcbbc9"><span class="section-number-3">4.2</span> TP2: avec une couche cachée</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Avec l'ajout d'une couche cachée, nous allons décomposer le calcul
en plusieurs étapes. Pour la propagation <i>forward</i>
</p>
<ul class="org-ul">
<li>\(\mathbf{a^{(1)}}=\mathbf{W^{(1)}}\mathbf{x}+\mathbf{b^{(1)}}\)</li>
<li>\(\mathbf{y^{(1)}}=f^{(1)}(\mathbf{a^{(1)}})\)</li>
<li>\(\mathbf{a^{(2)}}=\mathbf{W^{(2)}}\mathbf{y}^{(1)}+\mathbf{b^{(2)}}\)</li>
<li>\(\mathbf{y^{(2)}}=f^{(2)}(\mathbf{a^{(2)}})\)</li>
</ul>


<p>
Nous savons calculer le gradient de l'erreur par rapport à la couche
de sortie. Il faut maintenant <b>propager</b> le gradient de sortie jusqu'à
la couche cachée: 
</p>

<p>
\[
\mathbf{\delta^{(1)}} = \frac{\partial \mathcal{L}}{\partial
\mathbf{a^{(1)}}}
= \frac{\partial \mathcal{L}}{\partial\mathbf{a^{(2)}}} 
\frac{\partial \mathbf{a^{(2)}}}{\partial\mathbf{y^{(1)}}} 
\frac{\partial \mathbf{y^{(1)}}}{\partial\mathbf{a^{(1)}}} 
= \mathbf{\delta^{(1)}} \frac{\partial \mathbf{a^{(2)}}}{\partial\mathbf{y^{(1)}}} 
\frac{\partial \mathbf{y^{(1)}}}{\partial\mathbf{a^{(1)}}} 
\]
</p>

<p>
Ainsi il faut implémenter les deux dérivées partielles
"nouvelles". Une dépend du choix de la fonction d'activation. 
</p>
</div>
</div>


<div id="outline-container-org641b530" class="outline-3">
<h3 id="org641b530"><span class="section-number-3">4.3</span> TP2: Roadmap</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Les choses à faire pour ce TP sont: 
</p>
<ul class="org-ul">
<li>écrire les fonctions qui manquent pour mettre en pratique la
back-propagation du gradient;</li>
<li>ensuite, créer un test essentiel à la vérification de votre
implémentation: le test des différences finies.</li>
</ul>
<p>
\[
\frac{\partial g(\mathbf{w})}{\partial w_{i}} \approx
\frac{g(\mathbf{w} + \epsilon \mathbf{e}_{i}) -g(\mathbf{w} - \epsilon
\mathbf{e}_{i})   }{2 \epsilon}, 
\]
avec \(\mathbf{e}_{i}\) le ième vecteur de la base canonique de l'espace euclidien.
</p>
<ul class="org-ul">
<li>effectuer un apprentissage sur les données MNIST compléte et
comparer les résultats avec et sans couche cachée.</li>
</ul>
</div>
</div>
</div>
