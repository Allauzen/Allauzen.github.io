---
title: "Deep-Learning,  Master AIC"
date: 2018-10-17
layout: post
categories: 
tags: 
- AIC18 
- cours 
- NNet
published: true
comments: 
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org4735ebc">1. Cours 1 : 03/10, double séance 3h sur les bases du deep-learning</a></li>
<li><a href="#orge916df3">2. TP 1 : 09/10, premier pas</a></li>
<li><a href="#org7ebe8b7">3. Cours 2 : 10/10</a></li>
<li><a href="#orgd0b3062">4. TP2 : le 16/10, 1 couche cachée et back-prop</a>
<ul>
<li><a href="#org0e10feb">4.1. Récapitulation du TP1: sans couche cachée</a></li>
<li><a href="#org6bcbbc9">4.2. TP2: avec une couche cachée</a></li>
<li><a href="#org641b530">4.3. TP2: Roadmap</a></li>
<li><a href="#org41501f8">4.4. Travail à rendre pour le 23/10</a></li>
<li><a href="#orgfccf2be">4.5. Remarques</a></li>
</ul>
</li>
<li><a href="#org0f964ca">5. TP3: le 23/10 : Sur-apprentissage et régularisation</a>
<ul>
<li><a href="#org4973369">5.1. Sur-apprentissage</a></li>
<li><a href="#org7c425e0">5.2. Régularisation L2</a></li>
<li><a href="#orga40aa8e">5.3. Drop-out</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr>


<p>
Cours fait avec Michèle Sebag. Ce site contient les infos sur ma
(première) partie. 
</p>


<div id="outline-container-org4735ebc" class="outline-2">
<h2 id="org4735ebc"><span class="section-number-2">1</span> Cours 1 : 03/10, double séance 3h sur les bases du deep-learning</h2>
<div class="outline-text-2" id="text-1">
<p>
Pour les premiers cours, nous nous appuierons sur le cours en ligne de Hugo Larochelle.
La page officielle du cours est :
<a href="http://www.dmi.usherb.ca/~larocheh/cours/ift725_A2014/contenu.html">http://www.dmi.usherb.ca/~larocheh/cours/ift725_A2014/contenu.html</a>
</p>

<p>
Les vidéos associées sont accessibles via YouTube:
<a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH</a>
</p>


<p>
Pour la première séance, il est demandé de suivre AVANT les vidéos
numérotées de 1.1 (Artificial neuron) à 2.3 (output layer gradient).
Le travail à faire est donc d'avoir suivit ces cours: regarder et
prendre des notes comme pour un cours "habituel".  La séance du 03/10
partira du principe que ces vidéos ont été visionnées et sera dédiée à
vos questions et à des exercices.
</p>

<p>
Bon visionnage ! 
</p>
</div>
</div>


<div id="outline-container-orge916df3" class="outline-2">
<h2 id="orge916df3"><span class="section-number-2">2</span> TP 1 : 09/10, premier pas</h2>
<div class="outline-text-2" id="text-2">
<p>
Le TP est diponible via un notebook sur le drive habituel 
(<a href="https://drive.google.com/drive/folders/0B6bFVfow2ez_MjV3cEszTU05XzQ">rappel du lien</a>). Pour simplifier les choses, un module python
l'accompagne et il concerne le chargement des données. 
</p>
</div>
</div>


<div id="outline-container-org7ebe8b7" class="outline-2">
<h2 id="org7ebe8b7"><span class="section-number-2">3</span> Cours 2 : 10/10</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Faire les exercices qui sont à la fin des slides</li>
<li>Finir de voir les vidéos jusqu'à la 2.11.</li>
</ul>
</div>
</div>


<div id="outline-container-orgd0b3062" class="outline-2">
<h2 id="orgd0b3062"><span class="section-number-2">4</span> TP2 : le 16/10, 1 couche cachée et back-prop</h2>
<div class="outline-text-2" id="text-4">
<p>
On poursuit le TP précédent en ajoutant une couche cachée et
implémenter la back-prop. 
</p>
</div>


<div id="outline-container-org0e10feb" class="outline-3">
<h3 id="org0e10feb"><span class="section-number-3">4.1</span> Récapitulation du TP1: sans couche cachée</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Les fonctions écrites: 
</p>
<ul class="org-ul">
<li><i>init</i> : initialisation des paramètres (\(\mathbf{W},\mathbf{b}\))</li>
<li><i>forward</i> : à partir d'une entrée \(\mathbf{x}\) calcul
\(\mathbf{a}=\mathbf{W}\mathbf{x}+\mathbf{b}\)</li>
<li><i>softmax</i> : calcul la version stable du softmax</li>
<li><i>grad-out</i> : calcul le gradient de la fonction de perte par rapport
à la pré-activation de sortie, soit</li>
</ul>
<p>
\[\mathbf{\delta} = \frac{\partial \mathcal{L}}{\partial \mathbf{a}}.\]
</p>

<p>
La mise à jour des paramètres se calcul alors à partir de
\(\mathbf{\delta}\). 
</p>
</div>
</div>


<div id="outline-container-org6bcbbc9" class="outline-3">
<h3 id="org6bcbbc9"><span class="section-number-3">4.2</span> TP2: avec une couche cachée</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Avec l'ajout d'une couche cachée, nous allons décomposer le calcul
en plusieurs étapes. Pour la propagation <i>forward</i>
</p>
<ul class="org-ul">
<li>\(\mathbf{a^{(1)}}=\mathbf{W^{(1)}}\mathbf{x}+\mathbf{b^{(1)}}\)</li>
<li>\(\mathbf{y^{(1)}}=f^{(1)}(\mathbf{a^{(1)}})\)</li>
<li>\(\mathbf{a^{(2)}}=\mathbf{W^{(2)}}\mathbf{y}^{(1)}+\mathbf{b^{(2)}}\)</li>
<li>\(\mathbf{y^{(2)}}=f^{(2)}(\mathbf{a^{(2)}})\)</li>
</ul>


<p>
Nous savons calculer le gradient de l'erreur par rapport à la couche
de sortie. Il faut maintenant <b>propager</b> le gradient de sortie jusqu'à
la couche cachée: 
</p>

<p>
\[
\mathbf{\delta^{(1)}} = \frac{\partial \mathcal{L}}{\partial
\mathbf{a^{(1)}}}
= \frac{\partial \mathcal{L}}{\partial\mathbf{a^{(2)}}} 
\frac{\partial \mathbf{a^{(2)}}}{\partial\mathbf{y^{(1)}}} 
\frac{\partial \mathbf{y^{(1)}}}{\partial\mathbf{a^{(1)}}} 
= \mathbf{\delta^{(1)}} \frac{\partial \mathbf{a^{(2)}}}{\partial\mathbf{y^{(1)}}} 
\frac{\partial \mathbf{y^{(1)}}}{\partial\mathbf{a^{(1)}}} 
\]
</p>

<p>
Ainsi il faut implémenter les deux dérivées partielles
"nouvelles". Une dépend du choix de la fonction d'activation. 
</p>
</div>
</div>


<div id="outline-container-org641b530" class="outline-3">
<h3 id="org641b530"><span class="section-number-3">4.3</span> TP2: Roadmap</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Les choses à faire pour ce TP sont: 
</p>
<ul class="org-ul">
<li>écrire les fonctions qui manquent pour mettre en pratique la
back-propagation du gradient;</li>
<li>ensuite, créer un test essentiel à la vérification de votre
implémentation: le test des différences finies.</li>
</ul>
<p>
\[
\frac{\partial g(\mathbf{w})}{\partial w_{i}} \approx
\frac{g(\mathbf{w} + \epsilon \mathbf{e}_{i}) -g(\mathbf{w} - \epsilon
\mathbf{e}_{i})   }{2 \epsilon}, 
\]
avec \(\mathbf{e}_{i}\) le ième vecteur de la base canonique de l'espace euclidien.
</p>
<ul class="org-ul">
<li>effectuer un apprentissage sur les données MNIST compléte et
comparer les résultats avec et sans couche cachée.</li>
</ul>
</div>
</div>

<div id="outline-container-org41501f8" class="outline-3">
<h3 id="org41501f8"><span class="section-number-3">4.4</span> Travail à rendre pour le 23/10</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Il est demander de rendre le code pour les TP 1 et 2, accompagné d'un
compte rendu. Ainsi vous pouvez au m'envoyer 1/2 notebook, ou du code
source mais dans ce cas accompagné d'un compte rendu au format pdf. 
</p>



<p>
L'objectif est pour vous de finaliser votre code et d'observer un
apprentissage satisfaisant, c-a-d: 
</p>
<ul class="org-ul">
<li>l'évolution de la fonction objectif lors de l'apprentissage doit
décroitre lorsqu'elle est mesurée sur les données d'apprentissage</li>
<li>un modèle avec une couche cachée doit faire mieux qu'un modèle sans
couche cachée sur les données d'apprentissage</li>
</ul>

<p>
Il est important donc tracer l'evolution de la fonction objectif et du
taux d'erreur (ou de précision) sur les données d'apprentissage et les
données de validation. 
</p>

<p>
La partie sur la vérification du gradient est optionnelle. 
</p>

<p>
Si vous avez des questions, n'hésitez pas à communiquer via le groupe
google. 
</p>
</div>
</div>


<div id="outline-container-orgfccf2be" class="outline-3">
<h3 id="orgfccf2be"><span class="section-number-3">4.5</span> Remarques</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Attention à la stabilité numérique ! En cas d'incohérence, pensez à
observer l'évolution lors de l'apprentissage des prédiction: 
</p>
<ul class="org-ul">
<li>les valeurs données par le soft-max</li>
<li>les valeurs données avant (la pré-activation de sortie)</li>
</ul>

<p>
Les causes fréquentes: 
</p>
<ul class="org-ul">
<li>une mauvaise initialisation des paramètres. Il faut des valeurs
faibles et proches de zéro (voir le cours 1)</li>
<li>un pas de gradient trop élevé</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org0f964ca" class="outline-2">
<h2 id="org0f964ca"><span class="section-number-2">5</span> TP3: le 23/10 : Sur-apprentissage et régularisation</h2>
<div class="outline-text-2" id="text-5">
<p>
Ce TP suppose que vous avez terminé le TP2. Le but est d'observé le phénomène de sur-apprentissage puis d'explorer des solutions. 
</p>
</div>

<div id="outline-container-org4973369" class="outline-3">
<h3 id="org4973369"><span class="section-number-3">5.1</span> Sur-apprentissage</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>Vérifier que le code du TP2 fonctionne bien (réseau avec 97% de précision sur l'entraînement)</li>
<li>Montrer à l'aide d'une figure la différence entre l'utilisation de la fonction sigmoïde et/ou ReLU sur différents réseaux, e.g., 128-128, 128-64-32-16, 256-128-64-32-16, 512-256-128-64-32-16</li>
<li>Montrer à l'aide d'une figure le phénomène de sur-apprentissage sur les expériences précédentes ou sur un réseau 800-800</li>
</ul>
</div>
</div>


<div id="outline-container-org7c425e0" class="outline-3">
<h3 id="org7c425e0"><span class="section-number-3">5.2</span> Régularisation L2</h3>
<div class="outline-text-3" id="text-5-2">
<p>
La première méthode de régularisation est le weight decay. Avant de se lancer dans le code, il est important de réfléchir:
</p>
<ul class="org-ul">
<li>écrire l'équation de la fonction objectif qui intègre le terme de régularisation L2</li>
<li>Calculer le gradient et observer l'impact de ce terme sur les mises à jour</li>
<li>expérimenter la régularisation L2</li>
<li>Que se passe-t-il avec la régularisation L1 ?</li>
</ul>
</div>
</div>


<div id="outline-container-orga40aa8e" class="outline-3">
<h3 id="orga40aa8e"><span class="section-number-3">5.3</span> Drop-out</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Proposer puis implémenter le drop-out. 
</p>
</div>
</div>
</div>
